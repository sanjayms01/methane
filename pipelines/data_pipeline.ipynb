{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PIPELINE (EXTRACTING METHANE, WEATHER AND CLIMATE ZONE DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run following pip installs and restart notebook\n",
    "# !pip install xarray\n",
    "# !pip install geopandas\n",
    "# !pip install shapely\n",
    "# !pip install netCDF4\n",
    "# # !conda install --y aiobotocore\n",
    "# # !conda install botocore\n",
    "# # REMEMBER TO RESTART NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s5p good until 1 day before  \n",
    "era5 good until 18-19 days before...\n",
    "\n",
    "\n",
    "\n",
    "nov 1st data available \n",
    "nov 19th."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All libraries proeprly loaded!! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import traceback\n",
    "import sys\n",
    "import subprocess\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "import boto3\n",
    "import botocore\n",
    "import datetime\n",
    "\n",
    "from collections import Counter\n",
    "try:\n",
    "    import argparse\n",
    "    import glob\n",
    "    import sklearn\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "#     import plotly.express as px\n",
    "    from descartes import PolygonPatch\n",
    "    from matplotlib import pyplot as plt #viz\n",
    "    import matplotlib.colors as colors #colors for viz\n",
    "    from matplotlib.patches import Polygon\n",
    "    from matplotlib.collections import PatchCollection\n",
    "    import xarray as xr #process NetCDF\n",
    "    import numpy as np\n",
    "    import pandas as pd #data manipulation\n",
    "    import matplotlib.gridspec as gridspec #create subplot\n",
    "    from glob import iglob #data access in file manager\n",
    "    from os.path import join \n",
    "    from functools import reduce #string manipulation\n",
    "    import itertools #dict manipulation\n",
    "    import matplotlib.patches as mpatches\n",
    "    import geojson\n",
    "    import json\n",
    "    import altair as alt \n",
    "\n",
    "    from datetime import datetime, timedelta\n",
    "    import time\n",
    "    import pytz\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    #GeoPandas\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point\n",
    "    from shapely.geometry.polygon import Polygon\n",
    "    \n",
    "except ModuleNotFoundError:\n",
    "\n",
    "    print('\\nModule import error', '\\n')\n",
    "    print(traceback.format_exc())\n",
    "\n",
    "else:\n",
    "    print('\\nAll libraries proeprly loaded!!', '\\n')\n",
    "\n",
    "### HELPER FUNCTIONS\n",
    "def getHumanTime(seconds):\n",
    "    '''Make seconds human readable'''\n",
    "    if seconds <= 1.0:\n",
    "        return '00:00:01'\n",
    "    \n",
    "    seconds = int(seconds)\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return '{:d}:{:02d}:{:02d}'.format(h, m, s) # Python 3\n",
    "\n",
    "\n",
    "def print_write(content, f_object, should_print=True):\n",
    "    '''This function will both, and write the content to a local file'''\n",
    "    if should_print: print(content)\n",
    "    if f_object:\n",
    "        f_object.write(content)\n",
    "        f_object.write('\\n')\n",
    "        \n",
    "# alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Date Range for Generating Dataset  \n",
    "\n",
    "Define date range (`start_dt` and `end_dt`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2021-10-24', '2021-10-25', '2021-10-26', '2021-10-27', '2021-10-28', '2021-10-29', '2021-10-30', '2021-10-31']\n",
      "\n",
      " \n",
      " Last row of current complete dataframe has date of:  2021-10-23\n",
      "start_dt should be:  2021-10-24 \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Define date range\n",
    "from datetime import timedelta, date\n",
    "\n",
    "start_dt = date(2021, 10, 24 )          #DEFINE START DATE HERE\n",
    "end_dt = date(2021, 10, 31)              #DEFINE END DATE HERE\n",
    "\n",
    "localtime = time.localtime(time.time())\n",
    "# print(\"Local current time :\", localtime)\n",
    "y = localtime.tm_year\n",
    "m = localtime.tm_mon\n",
    "d = localtime.tm_mday\n",
    "# n = 7                                 # number of days to look back\n",
    "# end_dt = date(y, m, d)              #If you want current date\n",
    "# start_dt = end_dt-timedelta(n)\n",
    "\n",
    "date_batches = []\n",
    "def daterange(date1, date2):\n",
    "    for n in range(int ((date2 - date1).days)+1):\n",
    "        yield date1+timedelta(n)\n",
    "for dt in daterange(start_dt, end_dt):\n",
    "    date_batches.append(dt.strftime(\"%Y-%m-%d\"))\n",
    "print(date_batches)                     #CHECK IF THIS LIST INCLUDES ALL DATES IN DEFINED DATERANGE\n",
    "\n",
    "\n",
    "#Read current dataframe from s3\n",
    "bucket = 'methane-capstone'\n",
    "subfolder = 'data/dt=latest'\n",
    "s3_path = bucket+'/'+subfolder \n",
    "file_name='data-zone-combined.parquet.gzip'\n",
    "data_location = 's3://{}/{}'.format(s3_path, file_name)\n",
    "current_df = pd.read_parquet(data_location)\n",
    "\n",
    "print(\"\\n \\n Last row of current complete dataframe has date of: \", current_df.time_utc.iloc[-1].strftime(\"%Y-%m-%d\"))\n",
    "print(\"start_dt should be: \", (current_df.time_utc.iloc[-1]+ timedelta(days=1)).strftime(\"%Y-%m-%d\"), \"\\n \\n\")\n",
    "\n",
    "\n",
    "if current_df.time_utc.iloc[-1].strftime(\"%Y-%m-%d\") in date_batches:\n",
    "    print(\"#######################################\")\n",
    "    print(\"########## WARNING ####################\")\n",
    "    print(\"#######################################\")\n",
    "    print(\"\")\n",
    "    print(\"Last row of dataframe is within the date range specified above, please correct date range before running this notebook!\")\n",
    "    print(\"Or else, we could have duplicate data in dataframe!\")\n",
    "    print(\"\")\n",
    "    print(\"#######################################\")\n",
    "    print(\"########## END WARNING ################\")\n",
    "    print(\"#######################################\")\n",
    "\n",
    "del current_df  #delete variable after task complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methane S5P Extraction\n",
    "Goal: To extract methane reading data from Sentinel 5P via Amazon AWS public database.  \n",
    "Steps:  \n",
    "1. Specify Region of Interest, current region is California  \n",
    "2. Download whole batch (each date worth of .NC files) to local.    \n",
    "3. Parse each file in batch, and build a dataframe for each batch  \n",
    "4. For every batch we will flush the data to a `.parquet.gzip` straight to S3 in respective folder.\n",
    "    * Format `{date}_meth.parquet.gzip`\n",
    "6. Delete contents inside Sagemaker batch folder where `.nc` files were downloaded for a respective batch\n",
    "7. Write out a `.txt` file that writes out all the steps that happened in the pipeline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_s5p_extraction(date_batches):  \n",
    "    #Define Region of Interest, California\n",
    "    ca_geo_path = \"/root/methane/pipelines/resources/california.geojson\"\n",
    "    ca_geo_file = open(ca_geo_path)\n",
    "    ca_gpd = gpd.read_file(ca_geo_file)\n",
    "    cali_polygon = ca_gpd['geometry'][0]\n",
    "\n",
    "    #### HELPER FUNCTIONS ####\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    def getNCFile(date, local_path, outF):\n",
    "        '''Function to get a S5P file from AWS S3 to local'''\n",
    "        start = time.time()\n",
    "        try:\n",
    "            # Copy all the files from the entire folder for specified date\n",
    "            year = date[:4]\n",
    "            month = date[5:7]\n",
    "            day = date[-2:]\n",
    "            command = ['aws','s3','cp', \n",
    "                       f's3://meeo-s5p/OFFL/L2__CH4___/{year}/{month}/{day}', \n",
    "                       local_path, '--recursive']\n",
    "            subprocess.check_output(command)\n",
    "        except:\n",
    "            print_write(f\"ISSUE GETTING: {batch_file_name}\", outF)\n",
    "        end = time.time()\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    def getInputFiles(local_path):\n",
    "        '''\n",
    "        Get list of input files stored on Sagemaker directory \n",
    "        (run after getNCFile helper function)\n",
    "        '''\n",
    "        input_files = sorted(list(iglob(join(local_path, '**', '*CH4*.nc' ), recursive=True)), reverse=True)\n",
    "        return input_files\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    def loadNCFiles(input_files, outF):\n",
    "        '''Use xarray to load .nc file S5p products'''    \n",
    "        start = time.time()\n",
    "        s5p_products = {}\n",
    "        for file_name in input_files:\n",
    "            name = file_name.split('/')[-1]\n",
    "            start_date, end_date = list(filter(lambda x: len(x) == 15, file_name.split(\"_\")))\n",
    "            key = start_date + '::' + end_date\n",
    "            try:\n",
    "                #Open product - PRODUCT\n",
    "                with xr.load_dataset(file_name, group='PRODUCT') as s5p_prod:\n",
    "                    s5p_products[key] = s5p_prod\n",
    "                s5p_prod.close()\n",
    "            except:\n",
    "                print_write(f\"loadNCFiles - FAILED: {key}\", outF)\n",
    "        end = time.time()\n",
    "        return s5p_products, getHumanTime(end-start)\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    def filter_in_cali(row):\n",
    "        '''Filter apply function for CA'''\n",
    "        point = Point(row['lon'], row['lat'])\n",
    "        return cali_polygon.contains(point)\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    def processFiles(s5p_products, qa_thresh, outF):\n",
    "        '''Process all files that are in keys in `s5p_products` '''\n",
    "        columns = ['time_utc', \n",
    "                   'lat', 'lon', \n",
    "                   'rn_lat_1','rn_lon_1',\n",
    "                   'rn_lat_2','rn_lon_2',\n",
    "                   'rn_lat_5','rn_lon_5',\n",
    "                   'rn_lat','rn_lon',\n",
    "                   'qa_val', \n",
    "                   'methane_mixing_ratio',\n",
    "                   'methane_mixing_ratio_precision',\n",
    "                   'methane_mixing_ratio_bias_corrected']\n",
    "        df_ca = pd.DataFrame(columns=columns)\n",
    "\n",
    "        init_start = time.time()\n",
    "\n",
    "        first_orbit = 0        #keep track of the first orbit with data in California\n",
    "        last_orbit  = 0        #keep track of the last orbit with data in California, stop extracting other orbits afterwards\n",
    "\n",
    "        for file_number, product_key in enumerate(s5p_products, 1):\n",
    "\n",
    "            # If the last orbit to scan California has been searched,\n",
    "            # stop extract data from rest of the orbit data\n",
    "            # Note: there are 14 orbits per day\n",
    "            if last_orbit == 1:\n",
    "                break\n",
    "\n",
    "            start_time = time.time()\n",
    "            s5p_prod = s5p_products[product_key]\n",
    "            df_cur_scan = pd.DataFrame(columns=columns)\n",
    "            times_utc = np.array(s5p_prod.time_utc[0, :])\n",
    "\n",
    "            print_write(f'\\tfile_num: {file_number} - {product_key}. start_parse.', outF)\n",
    "\n",
    "            times_array = []\n",
    "            for utc, qa in zip(times_utc, s5p_prod.qa_value[0, :, :]):\n",
    "                times_array.extend([utc] * len(qa))\n",
    "\n",
    "            lats = np.array(s5p_prod.latitude[0, :, :]).ravel()\n",
    "            lons = np.array(s5p_prod.longitude[0, :, :]).ravel()\n",
    "            qa_vals = np.nan_to_num(s5p_prod.qa_value[0, :, :], nan=-9999).ravel()\n",
    "            methane_mixing_ratio = np.nan_to_num(s5p_prod.methane_mixing_ratio[0, :, :], nan=-9999).ravel()\n",
    "            methane_mixing_ratio_precision = np.nan_to_num(s5p_prod.methane_mixing_ratio_precision[0, :, :], nan=-9999).ravel()\n",
    "            methane_mixing_ratio_bias_corrected = np.nan_to_num(s5p_prod.methane_mixing_ratio_bias_corrected[0, :, :], nan=-9999).ravel()\n",
    "\n",
    "            cur_ts_dict = {\n",
    "                'time_utc' : times_array,\n",
    "                'lat' : lats,\n",
    "                'lon' : lons,\n",
    "                'rn_lat_1': np.round(lats, 1),\n",
    "                'rn_lon_1': np.round(lons, 1),\n",
    "                'rn_lat_2': np.round(lats*5)/5,\n",
    "                'rn_lon_2': np.round(lons*5)/5,\n",
    "                'rn_lat_5':  np.round(lats*2)/2,\n",
    "                'rn_lon_5': np.round(lons*2)/2,\n",
    "                'rn_lat': np.round(lats, 0),\n",
    "                'rn_lon': np.round(lons, 0),\n",
    "                'qa_val' : qa_vals,\n",
    "                'methane_mixing_ratio' : methane_mixing_ratio,\n",
    "                'methane_mixing_ratio_precision' : methane_mixing_ratio_precision,\n",
    "                'methane_mixing_ratio_bias_corrected' : methane_mixing_ratio_bias_corrected,\n",
    "            }\n",
    "\n",
    "            df_cur_ts = pd.DataFrame(cur_ts_dict)\n",
    "            df_cur_scan = pd.concat([df_cur_scan, df_cur_ts], ignore_index=True)   \n",
    "\n",
    "            #QA Mask\n",
    "            qa_mask_df = df_cur_scan['qa_val'] >= qa_thresh\n",
    "\n",
    "            #Methane Mask\n",
    "            meth_ca_mask_df = (df_cur_scan.methane_mixing_ratio != -9999) & \\\n",
    "                              (df_cur_scan.methane_mixing_ratio_precision != -9999) & \\\n",
    "                              (df_cur_scan.methane_mixing_ratio_bias_corrected != -9999)\n",
    "\n",
    "            #California Geo Mask\n",
    "            cali_mask = df_cur_scan.apply(filter_in_cali, axis=1)\n",
    "\n",
    "            #Join Masks\n",
    "            mask_join_df = qa_mask_df & cali_mask & meth_ca_mask_df\n",
    "            df_cur_scan_masked = df_cur_scan[mask_join_df]\n",
    "\n",
    "            df_ca = pd.concat([df_ca, df_cur_scan_masked], ignore_index=True)\n",
    "            end_time = time.time()\n",
    "            print_write(f'\\t\\t\\t\\t\\tend_parse. shape: {df_cur_scan_masked.shape}, time_taken: {getHumanTime(end_time-start_time)}', outF)\n",
    "\n",
    "            if df_cur_scan_masked.shape[0] > 0:\n",
    "                first_orbit = 1\n",
    "\n",
    "            if first_orbit == 1 and df_cur_scan_masked.shape[0] == 0:\n",
    "                last_orbit = 1\n",
    "                print(\"last orbit over region reached! start extracting next day!\")\n",
    "\n",
    "        return df_ca, getHumanTime(end_time-init_start)\n",
    "\n",
    "    ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### \n",
    "    ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### \n",
    "    ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### \n",
    "\n",
    "    #### CONFIG ####\n",
    "    local_pipe_log_path = '/root/methane/pipelines/resources/pipeline_runs/'\n",
    "    local_path = '/root/methane/pipelines/resources/nc_data/cur_batch/'\n",
    "    bucket = 'methane-capstone'\n",
    "    subfolder = 'data/pipeline-raw-data'\n",
    "    s3_path = bucket+'/'+subfolder\n",
    "\n",
    "    pipeline_start = True       #do we want to start this cell or not\n",
    "    delete_local_files = True   #do we want the local files deleted after each batch? or keep everything\n",
    "    qa_thresh = 0.0             #do we want to keep specific quality ratings of sp5 methane data?\n",
    "\n",
    "    ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### \n",
    "    ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### \n",
    "    ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### \n",
    "\n",
    "    #### S5P Pipeline ####\n",
    "\n",
    "    if pipeline_start:\n",
    "\n",
    "        #Give the pipeline log file a name\n",
    "        pipe_start = str(date_batches[0])   \n",
    "\n",
    "        try:\n",
    "            os.makedirs(local_pipe_log_path)\n",
    "        except:\n",
    "            print(\"pipeline logs folder already exist, we're good!\")\n",
    "\n",
    "        outF = open(f\"{local_pipe_log_path}pipe_log_{pipe_start}.txt\", \"w\")\n",
    "        print_write(\"######### PIPELINE STARTED #########\", outF)\n",
    "\n",
    "        init = time.time()    \n",
    "\n",
    "        try:\n",
    "            for batch_num, date in enumerate(date_batches, 1):\n",
    "\n",
    "                print_write(f\"\\nbatch_num: {batch_num} - start download\", outF)\n",
    "                batch_dn_start = time.time()\n",
    "\n",
    "                #Download all 14 .NC files on that one date\n",
    "                try:\n",
    "                    os.makedirs(local_path)\n",
    "                except:\n",
    "                    print(\"folder for raw nc files already exist, we're good!\")\n",
    "                getNCFile(date, local_path, outF)\n",
    "\n",
    "                batch_dn_end = time.time()\n",
    "                print_write(f\"batch_num: {batch_num} - finish download, time_taken: {getHumanTime(batch_dn_end-batch_dn_start)}\", outF)\n",
    "\n",
    "                #Get list of input files\n",
    "                input_files = getInputFiles(local_path)\n",
    "\n",
    "                #Load NC Files into dictionary\n",
    "                print_write(f\"batch_num: {batch_num} - start NC files load\", outF)\n",
    "                s5p_products, load_nc_time = loadNCFiles(input_files, outF)\n",
    "                print_write(f\"batch_num: {batch_num} - end NC files load, time_taken: {load_nc_time}\", outF)\n",
    "\n",
    "                #One entire batch is processsed into `cur_df`\n",
    "                #We name the `cur_df` as `cur_batch_df` for now\n",
    "                cur_batch_df, process_time = processFiles(s5p_products, qa_thresh, outF)\n",
    "\n",
    "                ### Write `cur_batch_df` for each batch to S3 ###\n",
    "                try:\n",
    "                    file_name=f'{date}_meth.parquet.gzip'\n",
    "                    cur_batch_df.to_parquet('s3://{}/{}'.format(s3_path, file_name), compression='gzip')\n",
    "\n",
    "                except:\n",
    "                    write_loc = 's3://{}/{}'.format(s3_path+f'/{year}', file_name)\n",
    "                    print_write(f\"ERROR S3 WRITE: {write_loc}\", outF)\n",
    "\n",
    "                if delete_local_files:\n",
    "                    for f in input_files:\n",
    "                        os.remove(f)\n",
    "                    print_write(\"deleted nc files\", outF)\n",
    "\n",
    "                batch_end_time = time.time()\n",
    "\n",
    "                print_write(f\"batch_num: {batch_num} batch_total_time: {getHumanTime(batch_end_time - batch_dn_start)} download_time: {getHumanTime(batch_dn_end-batch_dn_start)}, load_nc_time: {load_nc_time}, process_time: {process_time}, cur_batch_df_shape: {cur_batch_df.shape}\", outF)\n",
    "                print_write(\"#######################\", outF)\n",
    "            fin = time.time()\n",
    "            print_write(f\"\\nTOTAL PIPELINE TIME: {getHumanTime(fin-init)}\", outF)\n",
    "            outF.close()\n",
    "\n",
    "        except:\n",
    "            print_write(f\"\\n !!KABOOM!!\", outF)\n",
    "            outF.close()\n",
    "\n",
    "    else:\n",
    "        print(\"METHANE PIPELINE CLOSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather ERA5 Extraction  \n",
    "Goal: To extract weather data from ERA5 via Amazon AWS public database.  \n",
    "Steps:  \n",
    "Step 1: Extract all unique latitude/longitude combinations from Methane Dataframe. (Used to extract ERA5 data).  \n",
    "Step 2: Inspect AWS ERA5 directory and store relevant weather variable names.  \n",
    "Step 3: Download raw ERA5 data for weather variables and write to local directory.  \n",
    "Step 4: Extract only weather from lat/lon combinations from Methane Dataframe and write to local directory.\n",
    "Step 5: Merge weather data with methane data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_era5_extraction(date_batches):  \n",
    "\n",
    "    #Pipeline\n",
    "    delete_local_files = False  #set to false if you want to keep .NC raw data in local directory\n",
    "\n",
    "    #For each date in date range we are looking at\n",
    "    for date in date_batches:\n",
    "\n",
    "        #####################################################################################\n",
    "        #IMPORT METHANE DATA AND ONLY KEEP THE LAT/LON COMBINATIONS TO EXTRACT FROM ERA5 DATA\n",
    "        #####################################################################################\n",
    "\n",
    "        # Read in Methane Data\n",
    "        bucket = 'methane-capstone'\n",
    "        subfolder = 'data/pipeline-raw-data'\n",
    "        s3_path = bucket+'/'+subfolder\n",
    "        file_name=f'{date}_meth.parquet.gzip'\n",
    "        data_location = 's3://{}/{}'.format(s3_path, file_name)\n",
    "        methane_df = pd.read_parquet(data_location)\n",
    "        methane_df['time_utc'] = pd.to_datetime(methane_df['time_utc']).dt.tz_localize(None)\n",
    "        methane_df['time_utc_hour'] = methane_df['time_utc'].dt.round('h')\n",
    "\n",
    "        #Create list of lat/lon combinations\n",
    "        lat_lon = methane_df.groupby(['rn_lat_2', 'rn_lon_2']).count().reset_index()\n",
    "        locs = []\n",
    "        for lat,lon in zip(list(lat_lon['rn_lat_2']), list(lat_lon['rn_lon_2'])):\n",
    "            combined = {}\n",
    "            combined['name'] = str(lat)+'::'+str(lon)\n",
    "            combined['lon'] = lon\n",
    "            combined['lat'] = lat\n",
    "            locs.append(combined)            #Dictionary of dictionaries of name, longitude, and lat\n",
    "\n",
    "        #need to convert longitude in [-180,180] (typical range of longitude degrees) into [0,360] (format of ERA5 data)\n",
    "        for l in locs:\n",
    "            if l['lon'] < 0:\n",
    "                l['lon'] = 360 + l['lon']\n",
    "\n",
    "        # create these date variables for later use\n",
    "        year = date[:4]\n",
    "        month = date[5:7]\n",
    "        date_str = date                                  #date string for naming purposes\n",
    "        import datetime\n",
    "        date = datetime.date(int(year), int(month), 1)   #datetime to extract weather data\n",
    "\n",
    "        #####################################################################################\n",
    "        #CHECK EXISTING ERA5 DATA AVAILABILITY\n",
    "        #####################################################################################\n",
    "\n",
    "        era5_bucket = 'era5-pds'\n",
    "\n",
    "        # No AWS keys required\n",
    "        client = boto3.client('s3', config=botocore.client.Config(signature_version=botocore.UNSIGNED))\n",
    "\n",
    "        #See what data is available for date\n",
    "        keys = []\n",
    "        var_name_list = []              #STORE ALL THE WEATHER VARIABLES IN THIS LIST\n",
    "        prefix = date.strftime('%Y/%m/')\n",
    "\n",
    "        response = client.list_objects_v2(Bucket=era5_bucket, Prefix=prefix)\n",
    "        response_meta = response.get('ResponseMetadata')\n",
    "\n",
    "        if response_meta.get('HTTPStatusCode') == 200:\n",
    "            contents = response.get('Contents')\n",
    "            if contents == None:\n",
    "                print(\"No objects are available for %s\" % date.strftime('%B, %Y'))\n",
    "            else:\n",
    "                for obj in contents:\n",
    "                    keys.append(obj.get('Key'))\n",
    "                print(\"There are %s objects available for %s\\n--\" % (len(keys), date.strftime('%B, %Y')))\n",
    "                for k in keys:\n",
    "                    print(k)\n",
    "                    var_name_list.append(k.split(\"/\")[-1].split('.')[0])\n",
    "        else:\n",
    "            print(\"There was an error with your request.\")\n",
    "\n",
    "        #We will skip these variables from our final data\n",
    "        var_name_list.remove('main')   #just meta data\n",
    "        var_name_list.remove('sea_surface_wave_from_direction')   #lots of Nan's when joined (since we're focused on california)\n",
    "        var_name_list.remove('sea_surface_wave_mean_period')     #lots of Nan's when joined (since we're focused on california)\n",
    "        var_name_list.remove('sea_surface_temperature')     #lots of Nan's when joined (since we're focused on california)\n",
    "        var_name_list.remove('significant_height_of_wind_and_swell_waves')  #lots of Nan's when joined (since we're focused on california)\n",
    "\n",
    "        #this try is for data before 2019. The format includes a _meta file for EACH variable which is not needed, therefore, remove from our search\n",
    "        try: \n",
    "            meta_names = ['air_pressure_at_mean_sea_level_meta',\n",
    "                         'air_temperature_at_2_metres_1hour_Maximum_meta', 'air_temperature_at_2_metres_1hour_Minimum_meta',\n",
    "                         'air_temperature_at_2_metres_meta',\n",
    "                         'dew_point_temperature_at_2_metres_meta',\n",
    "                         'eastward_wind_at_100_metres_meta', 'eastward_wind_at_10_metres_meta',\n",
    "                         'integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation_meta',\n",
    "                         'lwe_thickness_of_surface_snow_amount_meta',\n",
    "                         'northward_wind_at_100_metres_meta', 'northward_wind_at_10_metres_meta',\n",
    "                         'precipitation_amount_1hour_Accumulation_meta',\n",
    "                         'sea_surface_temperature_meta',\n",
    "                         'significant_height_of_wind_and_swell_waves_meta',\n",
    "                         'snow_density_meta', 'surface_air_pressure_meta',]\n",
    "            for meta in meta_names:\n",
    "                var_name_list.remove(meta)\n",
    "\n",
    "        except:\n",
    "            print(\"\")\n",
    "    #         print(\"file names do not have an extra word 'meta' in it, we are good to go!\")   \n",
    "\n",
    "        print(\"List of variables to download:\", var_name_list)  #updated variable list\n",
    "        print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "        #####################################################################################\n",
    "        #DOWNLOAD RAW ERA5 DATA AND \n",
    "        #WRITE DATAFRAME FOR EACH WEATHER VARIABLE AT SPECIFIED LAT/LON COMBOS FOR ONE DATE\n",
    "        #####################################################################################    \n",
    "        start=time.time()\n",
    "\n",
    "        #For each weather variable\n",
    "        for var in var_name_list:\n",
    "\n",
    "            start2=time.time()\n",
    "            var=str(var)\n",
    "\n",
    "            try:\n",
    "                os.makedirs('/root/methane/pipelines/resources/weather_data')\n",
    "            except:\n",
    "                print(\"folder for storing weather data already exist, we're good!\")\n",
    "\n",
    "            #####################################################################################\n",
    "            #DOWNLOAD DATA FROM AWS BUCKET\n",
    "            #####################################################################################\n",
    "\n",
    "            # file path patterns for remote S3 objects and corresponding local file\n",
    "            s3_data_ptrn = '{year}/{month}/data/{var}.nc'\n",
    "            data_file_ptrn = '/root/methane/pipelines/resources/weather_data/{year}{month}_{var}.nc'\n",
    "\n",
    "            year = date.strftime('%Y')\n",
    "            month = date.strftime('%m')\n",
    "            s3_data_key = s3_data_ptrn.format(year=year, month=month, var=var)\n",
    "            data_file = data_file_ptrn.format(year=year, month=month, var=var)\n",
    "\n",
    "            if not os.path.isfile(data_file): # check if file already exists, if not, download\n",
    "                print(\"Downloading %s from S3...\" % s3_data_key)\n",
    "                client.download_file(era5_bucket, s3_data_key, data_file)\n",
    "                print(\"time to download file (seconds): \", time.time()-start2)\n",
    "\n",
    "            ds = xr.open_dataset(data_file)\n",
    "\n",
    "            #####################################################################################\n",
    "            # TAKE ERA5 DATA AND EXTRACT DATA FROM LAT/LON COMBO\n",
    "            #####################################################################################\n",
    "\n",
    "            ds_locs = xr.Dataset()\n",
    "            # interate through the locations and create a dataset\n",
    "            # containing the weather values for each location\n",
    "            print(\"Start Extraction: \", var)\n",
    "            start3=time.time()\n",
    "            for l in locs:\n",
    "                name = l['name']+'_'+var\n",
    "                lon = l['lon']\n",
    "                lat = l['lat']\n",
    "                var_name = name\n",
    "\n",
    "                ds2 = ds.sel(lon=lon, lat=lat, method='nearest')\n",
    "\n",
    "                lon_attr = '%s_lon' % name\n",
    "                lat_attr = '%s_lat' % name\n",
    "\n",
    "                ds2.attrs[lon_attr] = ds2.lon.values.tolist()\n",
    "                ds2.attrs[lat_attr] = ds2.lat.values.tolist()\n",
    "                ds2 = ds2.rename({var : var_name}).drop(('lat', 'lon'))\n",
    "\n",
    "                ds_locs = xr.merge([ds_locs, ds2])\n",
    "\n",
    "            #####################################################################################\n",
    "            # CONVERT TO DATAFRAME AND WRITE TO PARQUET\n",
    "            # Format would be a column for dates\n",
    "            #####################################################################################\n",
    "\n",
    "            df=ds_locs.to_dataframe()\n",
    "            df.to_parquet('/root/methane/pipelines/resources/weather_data/{}_{}.parquet.gzip'.format(date.strftime('%Y_%m'),var), compression='gzip')\n",
    "            print(\"time to extract data (seconds): \", time.time()-start2)\n",
    "            print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "        print(\"time to download all variables (secs): \", time.time()-start)\n",
    "\n",
    "        ######################################\n",
    "        # MERGE WEATHER DATA WITH METHANE DATA\n",
    "        ######################################\n",
    "\n",
    "        print(\"Start merging weather data to methane data, for each date\")\n",
    "        merge_start = time.time()\n",
    "\n",
    "        #List of Variable Names:\n",
    "        variable_names = ['air_pressure_at_mean_sea_level',\n",
    "        'air_temperature_at_2_metres',\n",
    "        'air_temperature_at_2_metres_1hour_Maximum',\n",
    "        'air_temperature_at_2_metres_1hour_Minimum',\n",
    "        'dew_point_temperature_at_2_metres',\n",
    "        'eastward_wind_at_100_metres',\n",
    "        'eastward_wind_at_10_metres',\n",
    "        'lwe_thickness_of_surface_snow_amount',\n",
    "        'northward_wind_at_100_metres',\n",
    "        'northward_wind_at_10_metres',\n",
    "        'snow_density',\n",
    "        'surface_air_pressure',             \n",
    "        'integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation',\n",
    "        'precipitation_amount_1hour_Accumulation']\n",
    "\n",
    "        #These variables have a different dataframe format, need additional filtering\n",
    "        variable_names_additional_filter=['integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation',\n",
    "        'precipitation_amount_1hour_Accumulation', 'air_temperature_at_2_metres_1hour_Maximum','air_temperature_at_2_metres_1hour_Minimum' ]\n",
    "\n",
    "        #Define function for splitting column name (formated as latlong_variablename)\n",
    "        def split_lat(x):\n",
    "            return x.split('::')[0]\n",
    "        def split_lon(x):\n",
    "            return x.split('::')[1].split(\"_\")[0]\n",
    "\n",
    "        #Local data directory\n",
    "        local_path = '/root/methane/pipelines/resources/weather_data/'\n",
    "\n",
    "        #For each data variable, merge to methane data\n",
    "        for variable in variable_names:\n",
    "\n",
    "            print(\"merging variable: \", variable)\n",
    "\n",
    "            #READ WEATHER DF STORED LOCALLY, REFORMAT, MERGE WITH METHANE DATAFRAME, WRITE TO S3 DRIVE\n",
    "            file_name='{}_{}_'.format(year,month)+variable+'.parquet.gzip'\n",
    "            weather_df = pd.read_parquet(local_path+file_name).reset_index()\n",
    "\n",
    "            if variable in variable_names_additional_filter:\n",
    "                print('feature w/ additional filter')\n",
    "                weather_df=weather_df[weather_df.nv==0]\n",
    "                weather_df.drop(weather_df.columns[1:3],axis=1,inplace=True)  #drop nv and timebound columns     \n",
    "\n",
    "                # Melt columns from different latitudes and longitudes to only 1 column\n",
    "                weather_df_melt = pd.melt(weather_df, id_vars=['time1'], value_vars= weather_df.columns[1:],\n",
    "                                         value_name=file_name.split('.')[0][8:])\n",
    "\n",
    "                #Create lat and lon columns for joining to methane data\n",
    "                weather_df_melt['lat_w'] = weather_df_melt['variable'].apply(split_lat)\n",
    "                weather_df_melt['lon_w'] = weather_df_melt['variable'].apply(split_lon)\n",
    "                weather_df_melt[['lat_w','lon_w']] = weather_df_melt[['lat_w','lon_w']].apply(pd.to_numeric)  #need these to be floats\n",
    "\n",
    "                #Truncate datetime for methane dataframe and weather dataframe to 1 hour for merging\n",
    "                weather_df_melt['time0_hour'] = weather_df_melt['time1'].dt.round('h')    \n",
    "\n",
    "                #Merge with Methane Data\n",
    "                methane_df = pd.merge(methane_df, weather_df_melt, how='left', left_on = ['time_utc_hour', 'rn_lat', 'rn_lon'], right_on = ['time0_hour','lat_w','lon_w'])\n",
    "                methane_df.drop(['time1','lat_w','lon_w', 'time0_hour','variable'], axis=1,inplace=True) #drop irrelvant columns\n",
    "\n",
    "            else: \n",
    "                print('feature w/o additional filter')\n",
    "                # Melt columns from different latitudes and longitudes to only 1 column\n",
    "                weather_df_melt = pd.melt(weather_df, id_vars=['time0'], value_vars= weather_df.columns[1:],\n",
    "                                         value_name=file_name.split('.')[0][8:])\n",
    "\n",
    "                #Create lat and lon columns for joining to methane data\n",
    "                weather_df_melt['lat_w'] = weather_df_melt['variable'].apply(split_lat)\n",
    "                weather_df_melt['lon_w'] = weather_df_melt['variable'].apply(split_lon)\n",
    "                weather_df_melt[['lat_w','lon_w']] = weather_df_melt[['lat_w','lon_w']].apply(pd.to_numeric)  #need these to be floats\n",
    "\n",
    "                #Truncate datetime for methane dataframe and weather dataframe to 1 hour for merging\n",
    "                weather_df_melt['time0_hour'] = weather_df_melt['time0'].dt.round('h')\n",
    "\n",
    "                #Merge with Methane Data\n",
    "                methane_df = pd.merge(methane_df, weather_df_melt, how='left', left_on = ['time_utc_hour', 'rn_lat', 'rn_lon'], right_on = ['time0_hour','lat_w','lon_w'])\n",
    "                methane_df.drop(['time0','lat_w','lon_w', 'time0_hour','variable'], axis=1,inplace=True) #drop irrelvant columns\n",
    "\n",
    "            #write methane_weather data\n",
    "    #         bucket = 'methane-capstone'\n",
    "    #         subfolder = 'data/pipeline-raw-data'\n",
    "    #         s3_path = bucket+'/'+subfolder\n",
    "            file_name=f'{date_str}_meth_weather.parquet.gzip'\n",
    "            methane_df.to_parquet('s3://{}/{}'.format(s3_path, file_name), compression='gzip')\n",
    "\n",
    "            print(\"date: {}, methane and weather data merged successfully in {} seconds\".format(date_str, (time.time()- merge_start)))\n",
    "\n",
    "    #CLEAN UP LOCAL DIRECTORY       \n",
    "    input_files1 = sorted(list(iglob(join(local_path, '**', '**.nc' ), recursive=True)), reverse=True)\n",
    "    input_files2 = sorted(list(iglob(join(local_path, '**', '**.gzip' ), recursive=True)), reverse=True)\n",
    "    input_files = input_files1+input_files2\n",
    "    if delete_local_files:\n",
    "        for f in input_files:\n",
    "            os.remove(f)\n",
    "        print(\"deleted weather files after all weather merge completed\")\n",
    "\n",
    "    print(\"WEATHER PIPELINE CLOSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CA Climate Zone Addition  \n",
    "Goal: Create column with CA climate zone regions for each row of methane dataframe. \n",
    "\n",
    "Understand Climate Zones\n",
    "\n",
    "https://cecgis-caenergy.opendata.arcgis.com/datasets/CAEnergy::california-building-climate-zones/explore?location=37.062390%2C-120.193659%2C5.99\n",
    "https://www.pge.com/includes/docs/pdfs/about/edusafety/training/pec/toolbox/arch/climate/california_climate_zones_01-16.pdf  \n",
    "\n",
    "Steps:  \n",
    "Step 1: Create lists of climate zone ID's and polygons.  \n",
    "Step 2: Create lists of all lat/lon combinations from the methane dataframe. Convert to a \"Point\".  \n",
    "Step 3: For each \"Point\", search if point is in a climate zone, if so, save that climate zone and add to methane dataframe.  \n",
    "Step 4: Save to S3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_CACZ_addition(date_batches):  \n",
    "    \n",
    "    #Pipeline\n",
    "    #Read CA climate zone data\n",
    "    file_name = '/root/methane/pipelines/resources/ca_building_climate_zones.geojson'\n",
    "    cl_gdf = gpd.read_file(file_name)\n",
    "\n",
    "    #For each date in date range we are looking at\n",
    "    for date in date_batches:\n",
    "\n",
    "        start=time.time()\n",
    "        print(\"Begin adding regions to date: {}\".format(date))\n",
    "\n",
    "        # Read in Methane Weather Data\n",
    "        bucket = 'methane-capstone'\n",
    "        subfolder = 'data/pipeline-raw-data'\n",
    "        s3_path = bucket+'/'+subfolder\n",
    "        file_name=f'{date}_meth_weather.parquet.gzip'\n",
    "        data_location = 's3://{}/{}'.format(s3_path, file_name)\n",
    "        df = pd.read_parquet(data_location)\n",
    "        df['time_utc'] = pd.to_datetime(df['time_utc']).dt.tz_localize(None)\n",
    "        df['time_utc_hour'] = df['time_utc'].dt.round('h')\n",
    "        print(\"Shape:\", df.shape)\n",
    "\n",
    "        #Step 1 Create mapping, list of all facilities polygons\n",
    "        zone_id_list = cl_gdf['BZone'].tolist()             #list of climate zones from the geopandas file\n",
    "        region_poly_list = cl_gdf['geometry'].tolist()      #list of polygons from the geopandas file\n",
    "\n",
    "        #Step 2 Create lists of lat/lon combinations and convert to a \"Point\"\n",
    "        lats = df['lat'].tolist()   #list of lats from methane df\n",
    "        lons = df['lon'].tolist()   #list of lons from methane df\n",
    "        def process_points(lon, lat):\n",
    "            return Point(lon, lat)\n",
    "        processed_points = [process_points(lons[i], lats[i]) for i in range(len(lats))]  #list of \"Points\" for each lat/lon in methane df\n",
    "\n",
    "        #Step 3 For each row, look if the row's \"Point\" is in a climate zone. If so, that climate zone is now added to a list associate for that row\n",
    "        point_zones = []   #List of climate zones for each row\n",
    "        for point in processed_points:\n",
    "            found=False\n",
    "            for i, poly in enumerate(region_poly_list, 0):\n",
    "                if poly.contains(point):\n",
    "                    point_zones.append(zone_id_list[i])\n",
    "                    found = True\n",
    "                    #If point has been found, no need to look at other polys\n",
    "                    break\n",
    "            if not found:   \n",
    "                point_zones.append(None)\n",
    "        df['BZone'] = point_zones     #Add list of climate zones for each row to a column in the methane df\n",
    "\n",
    "        #Step 4 Save to S3\n",
    "        file_name=f'{date}_meth_weather_region.parquet.gzip'\n",
    "        df.to_parquet('s3://{}/{}'.format(s3_path, file_name), compression='gzip')\n",
    "        print(\"Completed adding regions to date in {:.2f} seconds\".format(time.time()-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre Processing  \n",
    "Goal: Input each date's data and merge into one dataframe. Pre process data by grouping by date per CA Climate Zone.   \n",
    "\n",
    "Steps:  \n",
    "Step 1: Import each date's dataframe and groupby date for EACH zone.    \n",
    "Step 2: Merge grouped data for each zone into one dataframe (for each date).   \n",
    "Step 3: Merge each date's (non-grouped) data into one dataframe. Merge each date's grouped data into one dataframe.  \n",
    "Step 4: Save both dataframes to S3.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pre_process(date_batches):\n",
    "\n",
    "    #### HELPER FUNCTIONS ####\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    def get_time_resolution_groups(df):\n",
    "        bins = []\n",
    "        groups = []\n",
    "        df_reduced = df.groupby(df.time_utc.dt.date).agg({'methane_mixing_ratio': [\"count\",\"mean\"],\n",
    "                                                            'lat': [\"mean\"],\n",
    "                                                            'lat': [\"mean\"],\n",
    "                                                            'methane_mixing_ratio_precision':\"mean\",\n",
    "                                                            'methane_mixing_ratio_bias_corrected': \"mean\",\n",
    "                                                            'air_pressure_at_mean_sea_level': [\"mean\"],\n",
    "                                                            'air_temperature_at_2_metres': [\"mean\"],\n",
    "                                                            'air_temperature_at_2_metres_1hour_Maximum': [\"mean\"],\n",
    "                                                            'air_temperature_at_2_metres_1hour_Minimum': [\"mean\"],\n",
    "                                                            'dew_point_temperature_at_2_metres': [\"mean\"],\n",
    "                                                            'eastward_wind_at_100_metres': [\"mean\"],\n",
    "                                                            'eastward_wind_at_10_metres': [\"mean\"],\n",
    "                                                            'integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation' : [\"mean\"],\n",
    "                                                            'lwe_thickness_of_surface_snow_amount': [\"mean\"],\n",
    "                                                            'northward_wind_at_100_metres': [\"mean\"],\n",
    "                                                            'northward_wind_at_10_metres': [\"mean\"],\n",
    "                                                            'precipitation_amount_1hour_Accumulation': [\"mean\"],\n",
    "                                                            'snow_density': [\"mean\"],\n",
    "                                                            'surface_air_pressure': [\"mean\"],\n",
    "                                                            'qa_val': [('mode',lambda x:x.value_counts().index[0]), \"mean\"], #Numerical = Weight, Categorical = Mode\n",
    "                                                           })\n",
    "        #Flatten MultiIndex\n",
    "        df_reduced.columns = ['_'.join(col) for col in df_reduced.columns.values]\n",
    "        df_reduced = df_reduced.reset_index()\n",
    "        df_reduced = df_reduced.rename(columns={\"methane_mixing_ratio_count\": \"reading_count\"})\n",
    "        groups = [df_reduced]\n",
    "        bins = df_reduced['time_utc'].tolist()\n",
    "        return groups, bins\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    def get_processed_df(df, zone):\n",
    "\n",
    "        cur_df = df[df['BZone'] == zone]\n",
    "        groups, bins = get_time_resolution_groups(cur_df)\n",
    "        columns = groups[0].columns.tolist()\n",
    "        df_final = pd.DataFrame(columns=columns)\n",
    "\n",
    "        for ind, group in enumerate(groups, 1):\n",
    "            df_final = pd.concat([df_final, group])\n",
    "\n",
    "        df_final = df_final.sort_values('time_utc')\n",
    "        df_final['BZone'] = zone\n",
    "        df_final['BZone'] = df_final['BZone'].astype(int)\n",
    "        return df_final\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "\n",
    "\n",
    "    #Pipeline\n",
    "    bucket = 'methane-capstone'\n",
    "    subfolder = 'data/pipeline-raw-data'\n",
    "    s3_path = bucket+'/'+subfolder\n",
    "\n",
    "    grouped_combined = pd.DataFrame()   #for saving grouped by zone data\n",
    "    address_combined = pd.DataFrame()   #for saving all lat/lon data\n",
    "\n",
    "    for date in date_batches:\n",
    "\n",
    "        start=time.time()\n",
    "        print(\"Begin grouping data for date: {}\".format(date))\n",
    "\n",
    "        # Read in Methane Data\n",
    "        file_name=f'{date}_meth_weather_region.parquet.gzip'\n",
    "        data_location = 's3://{}/{}'.format(s3_path, file_name)\n",
    "        df = pd.read_parquet(data_location)\n",
    "        \n",
    "        #Combine non-grouped data into one dataframe\n",
    "        df_address = df\n",
    "        df_address['time_utc'] = pd.to_datetime(df_address['time_utc']).dt.tz_localize(None)   \n",
    "        if address_combined.shape[0] == 0:\n",
    "            address_combined = df_address\n",
    "        else:\n",
    "            address_combined = address_combined.append(df_address)   \n",
    "            \n",
    "        #Group data by zones and save to S3    \n",
    "        ZONES = ['1','2','3','4','5','6','7','8', \n",
    "                 '9','10','11','12','13','14','15','16']\n",
    "\n",
    "        regions_combined = pd.DataFrame()\n",
    "\n",
    "        for ZONE in ZONES:\n",
    "            df_cur = get_processed_df(df, ZONE)      \n",
    "            #combine each batch df into a single df and write to S3\n",
    "            if regions_combined.shape[0] == 0:\n",
    "                regions_combined = df_cur\n",
    "            else:\n",
    "                regions_combined = regions_combined.append(df_cur)\n",
    "\n",
    "        bucket = 'methane-capstone'\n",
    "        subfolder = 'data/pipeline-raw-data'\n",
    "        s3_path = bucket+'/'+subfolder\n",
    "        file_name=f'{date}_meth_weather_region_grouped.parquet.gzip'\n",
    "        regions_combined.to_parquet('s3://{}/{}'.format(s3_path, file_name), compression='gzip')\n",
    "        print(\"Completed grouping data for date in {:.2f} seconds\".format(time.time()-start))\n",
    "\n",
    "        #Combine grouped data into one dataframe\n",
    "        df_grouped = regions_combined\n",
    "        df_grouped['time_utc'] = pd.to_datetime(df_grouped['time_utc']).dt.tz_localize(None)\n",
    "\n",
    "        if grouped_combined.shape[0] == 0:\n",
    "            grouped_combined = df_grouped\n",
    "        else:\n",
    "            grouped_combined = grouped_combined.append(df_grouped)     \n",
    "            \n",
    "\n",
    "    grouped_combined = grouped_combined[['time_utc', 'BZone', 'reading_count', 'methane_mixing_ratio_mean',\n",
    "           'lat_mean', 'methane_mixing_ratio_precision_mean',\n",
    "           'methane_mixing_ratio_bias_corrected_mean',\n",
    "           'air_pressure_at_mean_sea_level_mean',\n",
    "           'air_temperature_at_2_metres_mean',\n",
    "           'air_temperature_at_2_metres_1hour_Maximum_mean',\n",
    "           'air_temperature_at_2_metres_1hour_Minimum_mean',\n",
    "           'dew_point_temperature_at_2_metres_mean',\n",
    "           'eastward_wind_at_100_metres_mean', 'eastward_wind_at_10_metres_mean',\n",
    "           'integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation_mean',\n",
    "           'lwe_thickness_of_surface_snow_amount_mean',\n",
    "           'northward_wind_at_100_metres_mean', 'northward_wind_at_10_metres_mean',\n",
    "           'precipitation_amount_1hour_Accumulation_mean', 'snow_density_mean',\n",
    "           'surface_air_pressure_mean', 'qa_val_mode', 'qa_val_mean']]\n",
    "    \n",
    "    address_combined = address_combined[['time_utc', 'lat', 'lon', 'rn_lat_1', 'rn_lon_1',\n",
    "       'rn_lat_2', 'rn_lon_2', 'rn_lat_5', 'rn_lon_5', 'rn_lat', 'rn_lon',\n",
    "       'qa_val', 'methane_mixing_ratio', 'methane_mixing_ratio_precision',\n",
    "       'methane_mixing_ratio_bias_corrected', 'time_utc_hour',\n",
    "       'air_pressure_at_mean_sea_level', 'air_temperature_at_2_metres',\n",
    "       'air_temperature_at_2_metres_1hour_Maximum',\n",
    "       'air_temperature_at_2_metres_1hour_Minimum',\n",
    "       'dew_point_temperature_at_2_metres', 'eastward_wind_at_100_metres',\n",
    "       'eastward_wind_at_10_metres',\n",
    "       'integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation',\n",
    "       'lwe_thickness_of_surface_snow_amount', 'northward_wind_at_100_metres',\n",
    "       'northward_wind_at_10_metres',\n",
    "       'precipitation_amount_1hour_Accumulation', 'snow_density',\n",
    "       'surface_air_pressure', 'BZone']]\n",
    "       \n",
    "    #save to s3\n",
    "    start = date_batches[0]\n",
    "    end = date_batches[-1]\n",
    "    file_name=f'merged_{start}_{end}_data-zone-combined.parquet.gzip'\n",
    "    grouped_combined.to_parquet('s3://{}/{}'.format(s3_path, file_name), compression='gzip')     \n",
    "    \n",
    "    file_name=f'merged_{start}_{end}_data-address-combined.parquet.gzip'\n",
    "    address_combined.to_parquet('s3://{}/{}'.format(s3_path, file_name), compression='gzip')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge to Final Dataframe\n",
    "Goal: Merge new downloaded data to final dataframe. \n",
    "\n",
    "Steps:  \n",
    "Step 1: Import existing final dataframe.  \n",
    "Step 2: Import pre-processed new data created recently from previous steps in this notebook.       \n",
    "Step 3: Check if all of the weather data columns are NaN's, if so, drop those rows (b/c ERA5 weather data hasn't been updated yet on Amazon Public Data Registry).    \n",
    "Step 4: Merge two dataframes.  \n",
    "Step 5: Save to S3 (replace old final dataframe and save to archive folder for backup).      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_final(date_batches):\n",
    "\n",
    "    #Import existing final dataframe \n",
    "    bucket = 'methane-capstone' #S3 bucket location\n",
    "\n",
    "    #Read current dataframes from s3\n",
    "    subfolder = 'data/dt=latest'\n",
    "    s3_path = bucket+'/'+subfolder \n",
    "    \n",
    "    #grouped data\n",
    "    file_name='data-zone-combined.parquet.gzip'\n",
    "    data_location = 's3://{}/{}'.format(s3_path, file_name)\n",
    "    current_df = pd.read_parquet(data_location)\n",
    "    \n",
    "    #non-grouped data\n",
    "    file_name='data-address-combined.parquet.gzip'\n",
    "    data_location = 's3://{}/{}'.format(s3_path, file_name)\n",
    "    current_df_address = pd.read_parquet(data_location)\n",
    "    \n",
    "    \n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    #Import pre-processed new data created recently from previous steps in this notebook\n",
    "    \n",
    "    start = date_batches[0]\n",
    "    end = date_batches[-1]\n",
    "    subfolder = 'data/pipeline-raw-data'\n",
    "    s3_path = bucket+'/'+subfolder \n",
    "    \n",
    "    #grouped data\n",
    "    file_name=f'merged_{start}_{end}_data-zone-combined.parquet.gzip'  #need to update, currently anmed the other way\n",
    "#     file_name=f'{start}_{end}_combined_data.parquet.gzip'\n",
    "    new_df = pd.read_parquet('s3://{}/{}'.format(s3_path, file_name))   \n",
    "    new_df['time_utc'] = pd.to_datetime(new_df['time_utc'])\n",
    "    \n",
    "    #non-grouped data\n",
    "    file_name=f'merged_{start}_{end}_data-address-combined.parquet.gzip'  #need to update, currently anmed the other way\n",
    "#     file_name=f'{start}_{end}_combined_data.parquet.gzip'\n",
    "    new_df_address = pd.read_parquet('s3://{}/{}'.format(s3_path, file_name))   \n",
    "    new_df_address['time_utc'] = pd.to_datetime(new_df_address['time_utc'])\n",
    "   \n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    #Check if all of the weather data columns are NaN's, if so, drop those rows (b/c ERA5 weather data hasn't been updated yet on Amazon Public Data Registry).\n",
    "\n",
    "    #new_df.columns[7:21] are weather data from ERA5. If *all* of these are NaN, then dropna with how='all' will drop these rows.\n",
    "    new_df = new_df[new_df['time_utc'] >= start].dropna(subset=new_df.columns[7:21], how='all')   #Drop NA's when all weather is missing\n",
    "    new_df_address = new_df_address[new_df_address['time_utc'] >= start].dropna(subset=new_df_address.columns[16:30], how='all')   #Drop NA's when all weather is missing\n",
    "\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    #Merge two dataframes.\n",
    "\n",
    "    assert sum(current_df.columns == new_df.columns) == len(current_df.columns)  #confirm that two dataframes have the same columns\n",
    "    assert sum(current_df_address.columns == new_df_address.columns) == len(current_df_address.columns)  #confirm that two dataframes have the same columns\n",
    "    # sum(df1.columns==df.2columns) tracks how many columns are the same between the two df\n",
    "    # len(df1.columns) trakcs total number of columns.  \n",
    "    # If the two sums are equal, all the columns are the same for df1 and df2.\n",
    "\n",
    "    final_merged_df = current_df.append(new_df)\n",
    "    final_merged_df['time_utc'] = pd.to_datetime(final_merged_df['time_utc'])\n",
    "    final_merged_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    final_merged_df_address = current_df_address.append(new_df_address)\n",
    "    final_merged_df_address['time_utc'] = pd.to_datetime(final_merged_df_address['time_utc'])\n",
    "    final_merged_df_address.drop_duplicates(inplace=True)\n",
    "\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    ##########################################################################################\n",
    "    \n",
    "    # Save to S3 (replace old final dataframe)\n",
    "    subfolder = 'data/dt=latest'\n",
    "    s3_path = bucket+'/'+subfolder \n",
    "\n",
    "    file_name='data-zone-combined.parquet.gzip'\n",
    "    final_merged_df.to_parquet('s3://{}/{}'.format(s3_path, file_name), compression='gzip')\n",
    "    data_location_csv_zone = 's3://methane-capstone/public_data/zone-data.csv'\n",
    "    final_merged_df.to_csv(data_location_csv_zone) #publicly available dataset   \n",
    "    \n",
    "    file_name='data-address-combined.parquet.gzip'\n",
    "    final_merged_df_address.to_parquet('s3://{}/{}'.format(s3_path, file_name), compression='gzip')    \n",
    "    data_location_csv_address = 's3://methane-capstone/public_data/full-raw-data.csv'\n",
    "    final_merged_df_address.to_csv(data_location_csv_address) #publicly available dataset       \n",
    "    \n",
    "    # Save to S3 (in archive folder for backup)\n",
    "#     date=str(localtime.tm_year)+str(localtime.tm_mon)+str(localtime.tm_mday) #define date (for naming backup)\n",
    "    subfolder = f'data/dt=archive/{start}_{end}'\n",
    "    s3_path = bucket+'/'+subfolder\n",
    "    \n",
    "    file_name=f'{end}_data-zone-combined.parquet.gzip'\n",
    "    final_merged_df.to_parquet('s3://{}/{}'.format(s3_path, file_name), compression='gzip')\n",
    "\n",
    "    file_name=f'{end}_data-address-combined.parquet.gzip'\n",
    "    final_merged_df_address.to_parquet('s3://{}/{}'.format(s3_path, file_name), compression='gzip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN PIPELINE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline logs folder already exist, we're good!\n",
      "######### PIPELINE STARTED #########\n",
      "\n",
      "batch_num: 1 - start download\n",
      "folder for raw nc files already exist, we're good!\n",
      "batch_num: 1 - finish download, time_taken: 0:00:12\n",
      "batch_num: 1 - start NC files load\n",
      "batch_num: 1 - end NC files load, time_taken: 0:00:06\n",
      "\tfile_num: 1 - 20211024T224244::20211025T002414. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:26\n",
      "\tfile_num: 2 - 20211024T210115::20211024T224244. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:27\n",
      "\tfile_num: 3 - 20211024T191945::20211024T210115. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (190, 15), time_taken: 0:00:30\n",
      "\tfile_num: 4 - 20211024T173816::20211024T191945. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:31\n",
      "last orbit over region reached! start extracting next day!\n",
      "deleted nc files\n",
      "batch_num: 1 batch_total_time: 0:02:14 download_time: 0:00:12, load_nc_time: 0:00:06, process_time: 0:01:55, cur_batch_df_shape: (190, 15)\n",
      "#######################\n",
      "\n",
      "batch_num: 2 - start download\n",
      "folder for raw nc files already exist, we're good!\n",
      "batch_num: 2 - finish download, time_taken: 0:00:13\n",
      "batch_num: 2 - start NC files load\n",
      "batch_num: 2 - end NC files load, time_taken: 0:00:06\n",
      "\tfile_num: 1 - 20211025T222337::20211026T000506. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:27\n",
      "\tfile_num: 2 - 20211025T204207::20211025T222337. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (46, 15), time_taken: 0:00:29\n",
      "\tfile_num: 3 - 20211025T190038::20211025T204207. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (41, 15), time_taken: 0:00:30\n",
      "\tfile_num: 4 - 20211025T171908::20211025T190038. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:26\n",
      "last orbit over region reached! start extracting next day!\n",
      "deleted nc files\n",
      "batch_num: 2 batch_total_time: 0:02:12 download_time: 0:00:13, load_nc_time: 0:00:06, process_time: 0:01:52, cur_batch_df_shape: (87, 15)\n",
      "#######################\n",
      "\n",
      "batch_num: 3 - start download\n",
      "folder for raw nc files already exist, we're good!\n",
      "batch_num: 3 - finish download, time_taken: 0:00:13\n",
      "batch_num: 3 - start NC files load\n",
      "batch_num: 3 - end NC files load, time_taken: 0:00:07\n",
      "\tfile_num: 1 - 20211026T234558::20211027T012728. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:26\n",
      "\tfile_num: 2 - 20211026T220429::20211026T234558. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:25\n",
      "\tfile_num: 3 - 20211026T202259::20211026T220429. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (1306, 15), time_taken: 0:00:32\n",
      "\tfile_num: 4 - 20211026T184130::20211026T202259. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:26\n",
      "last orbit over region reached! start extracting next day!\n",
      "deleted nc files\n",
      "batch_num: 3 batch_total_time: 0:02:12 download_time: 0:00:13, load_nc_time: 0:00:07, process_time: 0:01:51, cur_batch_df_shape: (1306, 15)\n",
      "#######################\n",
      "\n",
      "batch_num: 4 - start download\n",
      "folder for raw nc files already exist, we're good!\n",
      "batch_num: 4 - finish download, time_taken: 0:00:14\n",
      "batch_num: 4 - start NC files load\n",
      "batch_num: 4 - end NC files load, time_taken: 0:00:06\n",
      "\tfile_num: 1 - 20211027T232651::20211028T010820. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:26\n",
      "\tfile_num: 2 - 20211027T214521::20211027T232651. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:27\n",
      "\tfile_num: 3 - 20211027T200352::20211027T214521. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (2673, 15), time_taken: 0:00:33\n",
      "\tfile_num: 4 - 20211027T182222::20211027T200352. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:26\n",
      "last orbit over region reached! start extracting next day!\n",
      "deleted nc files\n",
      "batch_num: 4 batch_total_time: 0:02:15 download_time: 0:00:14, load_nc_time: 0:00:06, process_time: 0:01:54, cur_batch_df_shape: (2673, 15)\n",
      "#######################\n",
      "\n",
      "batch_num: 5 - start download\n",
      "folder for raw nc files already exist, we're good!\n",
      "batch_num: 5 - finish download, time_taken: 0:00:13\n",
      "batch_num: 5 - start NC files load\n",
      "batch_num: 5 - end NC files load, time_taken: 0:00:06\n",
      "\tfile_num: 1 - 20211028T230743::20211029T004913. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:26\n",
      "\tfile_num: 2 - 20211028T212614::20211028T230743. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:26\n",
      "\tfile_num: 3 - 20211028T194444::20211028T212614. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (3613, 15), time_taken: 0:00:32\n",
      "\tfile_num: 4 - 20211028T180315::20211028T194444. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:26\n",
      "last orbit over region reached! start extracting next day!\n",
      "deleted nc files\n",
      "batch_num: 5 batch_total_time: 0:02:12 download_time: 0:00:13, load_nc_time: 0:00:06, process_time: 0:01:52, cur_batch_df_shape: (3613, 15)\n",
      "#######################\n",
      "\n",
      "batch_num: 6 - start download\n",
      "folder for raw nc files already exist, we're good!\n",
      "batch_num: 6 - finish download, time_taken: 0:00:13\n",
      "batch_num: 6 - start NC files load\n",
      "batch_num: 6 - end NC files load, time_taken: 0:00:06\n",
      "\tfile_num: 1 - 20211029T224836::20211030T003005. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:27\n",
      "\tfile_num: 2 - 20211029T210706::20211029T224836. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:26\n",
      "\tfile_num: 3 - 20211029T192537::20211029T210706. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (1783, 15), time_taken: 0:00:30\n",
      "\tfile_num: 4 - 20211029T174407::20211029T192537. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:26\n",
      "last orbit over region reached! start extracting next day!\n",
      "deleted nc files\n",
      "batch_num: 6 batch_total_time: 0:02:11 download_time: 0:00:13, load_nc_time: 0:00:06, process_time: 0:01:51, cur_batch_df_shape: (1783, 15)\n",
      "#######################\n",
      "\n",
      "batch_num: 7 - start download\n",
      "folder for raw nc files already exist, we're good!\n",
      "batch_num: 7 - finish download, time_taken: 0:00:13\n",
      "batch_num: 7 - start NC files load\n",
      "batch_num: 7 - end NC files load, time_taken: 0:00:06\n",
      "\tfile_num: 1 - 20211030T222928::20211031T001058. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:26\n",
      "\tfile_num: 2 - 20211030T204759::20211030T222928. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (16, 15), time_taken: 0:00:28\n",
      "\tfile_num: 3 - 20211030T190629::20211030T204759. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (357, 15), time_taken: 0:00:29\n",
      "\tfile_num: 4 - 20211030T172500::20211030T190629. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:26\n",
      "last orbit over region reached! start extracting next day!\n",
      "deleted nc files\n",
      "batch_num: 7 batch_total_time: 0:02:11 download_time: 0:00:13, load_nc_time: 0:00:06, process_time: 0:01:51, cur_batch_df_shape: (373, 15)\n",
      "#######################\n",
      "\n",
      "batch_num: 8 - start download\n",
      "folder for raw nc files already exist, we're good!\n",
      "batch_num: 8 - finish download, time_taken: 0:00:13\n",
      "batch_num: 8 - start NC files load\n",
      "batch_num: 8 - end NC files load, time_taken: 0:00:07\n",
      "\tfile_num: 1 - 20211031T235150::20211101T013319. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:25\n",
      "\tfile_num: 2 - 20211031T221021::20211031T235150. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:26\n",
      "\tfile_num: 3 - 20211031T202851::20211031T221021. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (440, 15), time_taken: 0:00:31\n",
      "\tfile_num: 4 - 20211031T184722::20211031T202851. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:27\n",
      "last orbit over region reached! start extracting next day!\n",
      "deleted nc files\n",
      "batch_num: 8 batch_total_time: 0:02:13 download_time: 0:00:13, load_nc_time: 0:00:07, process_time: 0:01:51, cur_batch_df_shape: (440, 15)\n",
      "#######################\n",
      "\n",
      "TOTAL PIPELINE TIME: 0:17:44\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "Time to Finish Methane Data Extraction:  1064.5588762760162\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "There are 19 objects available for October, 2021\n",
      "--\n",
      "2021/10/data/air_pressure_at_mean_sea_level.nc\n",
      "2021/10/data/air_temperature_at_2_metres.nc\n",
      "2021/10/data/air_temperature_at_2_metres_1hour_Maximum.nc\n",
      "2021/10/data/air_temperature_at_2_metres_1hour_Minimum.nc\n",
      "2021/10/data/dew_point_temperature_at_2_metres.nc\n",
      "2021/10/data/eastward_wind_at_100_metres.nc\n",
      "2021/10/data/eastward_wind_at_10_metres.nc\n",
      "2021/10/data/integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation.nc\n",
      "2021/10/data/lwe_thickness_of_surface_snow_amount.nc\n",
      "2021/10/data/northward_wind_at_100_metres.nc\n",
      "2021/10/data/northward_wind_at_10_metres.nc\n",
      "2021/10/data/precipitation_amount_1hour_Accumulation.nc\n",
      "2021/10/data/sea_surface_temperature.nc\n",
      "2021/10/data/sea_surface_wave_from_direction.nc\n",
      "2021/10/data/sea_surface_wave_mean_period.nc\n",
      "2021/10/data/significant_height_of_wind_and_swell_waves.nc\n",
      "2021/10/data/snow_density.nc\n",
      "2021/10/data/surface_air_pressure.nc\n",
      "2021/10/main.nc\n",
      "\n",
      "List of variables to download: ['air_pressure_at_mean_sea_level', 'air_temperature_at_2_metres', 'air_temperature_at_2_metres_1hour_Maximum', 'air_temperature_at_2_metres_1hour_Minimum', 'dew_point_temperature_at_2_metres', 'eastward_wind_at_100_metres', 'eastward_wind_at_10_metres', 'integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation', 'lwe_thickness_of_surface_snow_amount', 'northward_wind_at_100_metres', 'northward_wind_at_10_metres', 'precipitation_amount_1hour_Accumulation', 'snow_density', 'surface_air_pressure']\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  air_pressure_at_mean_sea_level\n",
      "time to extract data (seconds):  11.670636177062988\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  air_temperature_at_2_metres\n",
      "time to extract data (seconds):  9.009004592895508\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  air_temperature_at_2_metres_1hour_Maximum\n",
      "time to extract data (seconds):  9.100141286849976\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  air_temperature_at_2_metres_1hour_Minimum\n",
      "time to extract data (seconds):  9.234987258911133\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  dew_point_temperature_at_2_metres\n",
      "time to extract data (seconds):  9.703931331634521\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  eastward_wind_at_100_metres\n",
      "time to extract data (seconds):  9.823694467544556\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  eastward_wind_at_10_metres\n",
      "time to extract data (seconds):  10.436917781829834\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation\n",
      "time to extract data (seconds):  8.899330615997314\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  lwe_thickness_of_surface_snow_amount\n",
      "time to extract data (seconds):  6.149690628051758\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  northward_wind_at_100_metres\n",
      "time to extract data (seconds):  9.731727123260498\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  northward_wind_at_10_metres\n",
      "time to extract data (seconds):  9.68306016921997\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  precipitation_amount_1hour_Accumulation\n",
      "time to extract data (seconds):  7.120142936706543\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  snow_density\n",
      "time to extract data (seconds):  6.137671709060669\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  surface_air_pressure\n",
      "time to extract data (seconds):  10.491342782974243\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "time to download all variables (secs):  127.19695258140564\n",
      "Start merging weather data to methane data, for each date\n",
      "merging variable:  air_pressure_at_mean_sea_level\n",
      "feature w/o additional filter\n",
      "date: 2021-10-24, methane and weather data merged successfully in 0.25368499755859375 seconds\n",
      "merging variable:  air_temperature_at_2_metres\n",
      "feature w/o additional filter\n",
      "date: 2021-10-24, methane and weather data merged successfully in 0.42165040969848633 seconds\n",
      "merging variable:  air_temperature_at_2_metres_1hour_Maximum\n",
      "feature w/ additional filter\n",
      "date: 2021-10-24, methane and weather data merged successfully in 0.6005387306213379 seconds\n",
      "merging variable:  air_temperature_at_2_metres_1hour_Minimum\n",
      "feature w/ additional filter\n",
      "date: 2021-10-24, methane and weather data merged successfully in 0.7820749282836914 seconds\n",
      "merging variable:  dew_point_temperature_at_2_metres\n",
      "feature w/o additional filter\n",
      "date: 2021-10-24, methane and weather data merged successfully in 0.9612712860107422 seconds\n",
      "merging variable:  eastward_wind_at_100_metres\n",
      "feature w/o additional filter\n",
      "date: 2021-10-24, methane and weather data merged successfully in 1.1950557231903076 seconds\n",
      "merging variable:  eastward_wind_at_10_metres\n",
      "feature w/o additional filter\n",
      "date: 2021-10-24, methane and weather data merged successfully in 1.3990552425384521 seconds\n",
      "merging variable:  lwe_thickness_of_surface_snow_amount\n",
      "feature w/o additional filter\n",
      "date: 2021-10-24, methane and weather data merged successfully in 1.5753321647644043 seconds\n",
      "merging variable:  northward_wind_at_100_metres\n",
      "feature w/o additional filter\n",
      "date: 2021-10-24, methane and weather data merged successfully in 1.7484042644500732 seconds\n",
      "merging variable:  northward_wind_at_10_metres\n",
      "feature w/o additional filter\n",
      "date: 2021-10-24, methane and weather data merged successfully in 1.925473690032959 seconds\n",
      "merging variable:  snow_density\n",
      "feature w/o additional filter\n",
      "date: 2021-10-24, methane and weather data merged successfully in 2.134077310562134 seconds\n",
      "merging variable:  surface_air_pressure\n",
      "feature w/o additional filter\n",
      "date: 2021-10-24, methane and weather data merged successfully in 2.3019766807556152 seconds\n",
      "merging variable:  integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation\n",
      "feature w/ additional filter\n",
      "date: 2021-10-24, methane and weather data merged successfully in 2.546734571456909 seconds\n",
      "merging variable:  precipitation_amount_1hour_Accumulation\n",
      "feature w/ additional filter\n",
      "date: 2021-10-24, methane and weather data merged successfully in 2.7804272174835205 seconds\n",
      "There are 19 objects available for October, 2021\n",
      "--\n",
      "2021/10/data/air_pressure_at_mean_sea_level.nc\n",
      "2021/10/data/air_temperature_at_2_metres.nc\n",
      "2021/10/data/air_temperature_at_2_metres_1hour_Maximum.nc\n",
      "2021/10/data/air_temperature_at_2_metres_1hour_Minimum.nc\n",
      "2021/10/data/dew_point_temperature_at_2_metres.nc\n",
      "2021/10/data/eastward_wind_at_100_metres.nc\n",
      "2021/10/data/eastward_wind_at_10_metres.nc\n",
      "2021/10/data/integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation.nc\n",
      "2021/10/data/lwe_thickness_of_surface_snow_amount.nc\n",
      "2021/10/data/northward_wind_at_100_metres.nc\n",
      "2021/10/data/northward_wind_at_10_metres.nc\n",
      "2021/10/data/precipitation_amount_1hour_Accumulation.nc\n",
      "2021/10/data/sea_surface_temperature.nc\n",
      "2021/10/data/sea_surface_wave_from_direction.nc\n",
      "2021/10/data/sea_surface_wave_mean_period.nc\n",
      "2021/10/data/significant_height_of_wind_and_swell_waves.nc\n",
      "2021/10/data/snow_density.nc\n",
      "2021/10/data/surface_air_pressure.nc\n",
      "2021/10/main.nc\n",
      "\n",
      "List of variables to download: ['air_pressure_at_mean_sea_level', 'air_temperature_at_2_metres', 'air_temperature_at_2_metres_1hour_Maximum', 'air_temperature_at_2_metres_1hour_Minimum', 'dew_point_temperature_at_2_metres', 'eastward_wind_at_100_metres', 'eastward_wind_at_10_metres', 'integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation', 'lwe_thickness_of_surface_snow_amount', 'northward_wind_at_100_metres', 'northward_wind_at_10_metres', 'precipitation_amount_1hour_Accumulation', 'snow_density', 'surface_air_pressure']\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  air_pressure_at_mean_sea_level\n",
      "time to extract data (seconds):  7.7750630378723145\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  air_temperature_at_2_metres\n",
      "time to extract data (seconds):  6.147357702255249\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  air_temperature_at_2_metres_1hour_Maximum\n",
      "time to extract data (seconds):  6.230198383331299\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  air_temperature_at_2_metres_1hour_Minimum\n",
      "time to extract data (seconds):  6.311253070831299\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  dew_point_temperature_at_2_metres\n",
      "time to extract data (seconds):  6.478436708450317\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  eastward_wind_at_100_metres\n",
      "time to extract data (seconds):  6.892682313919067\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  eastward_wind_at_10_metres\n",
      "time to extract data (seconds):  6.706987619400024\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation\n",
      "time to extract data (seconds):  6.8595921993255615\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  lwe_thickness_of_surface_snow_amount\n",
      "time to extract data (seconds):  4.180384159088135\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  northward_wind_at_100_metres\n",
      "time to extract data (seconds):  6.618335247039795\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  northward_wind_at_10_metres\n",
      "time to extract data (seconds):  6.598059892654419\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  precipitation_amount_1hour_Accumulation\n",
      "time to extract data (seconds):  4.68753719329834\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  snow_density\n",
      "time to extract data (seconds):  4.314183235168457\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  surface_air_pressure\n",
      "time to extract data (seconds):  6.531883001327515\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "time to download all variables (secs):  86.33543539047241\n",
      "Start merging weather data to methane data, for each date\n",
      "merging variable:  air_pressure_at_mean_sea_level\n",
      "feature w/o additional filter\n",
      "date: 2021-10-25, methane and weather data merged successfully in 0.16634631156921387 seconds\n",
      "merging variable:  air_temperature_at_2_metres\n",
      "feature w/o additional filter\n",
      "date: 2021-10-25, methane and weather data merged successfully in 0.34778332710266113 seconds\n",
      "merging variable:  air_temperature_at_2_metres_1hour_Maximum\n",
      "feature w/ additional filter\n",
      "date: 2021-10-25, methane and weather data merged successfully in 0.48923540115356445 seconds\n",
      "merging variable:  air_temperature_at_2_metres_1hour_Minimum\n",
      "feature w/ additional filter\n",
      "date: 2021-10-25, methane and weather data merged successfully in 0.6480612754821777 seconds\n",
      "merging variable:  dew_point_temperature_at_2_metres\n",
      "feature w/o additional filter\n",
      "date: 2021-10-25, methane and weather data merged successfully in 0.7799568176269531 seconds\n",
      "merging variable:  eastward_wind_at_100_metres\n",
      "feature w/o additional filter\n",
      "date: 2021-10-25, methane and weather data merged successfully in 1.0177032947540283 seconds\n",
      "merging variable:  eastward_wind_at_10_metres\n",
      "feature w/o additional filter\n",
      "date: 2021-10-25, methane and weather data merged successfully in 1.1766057014465332 seconds\n",
      "merging variable:  lwe_thickness_of_surface_snow_amount\n",
      "feature w/o additional filter\n",
      "date: 2021-10-25, methane and weather data merged successfully in 1.374483346939087 seconds\n",
      "merging variable:  northward_wind_at_100_metres\n",
      "feature w/o additional filter\n",
      "date: 2021-10-25, methane and weather data merged successfully in 1.576362133026123 seconds\n",
      "merging variable:  northward_wind_at_10_metres\n",
      "feature w/o additional filter\n",
      "date: 2021-10-25, methane and weather data merged successfully in 1.7660059928894043 seconds\n",
      "merging variable:  snow_density\n",
      "feature w/o additional filter\n",
      "date: 2021-10-25, methane and weather data merged successfully in 1.9104785919189453 seconds\n",
      "merging variable:  surface_air_pressure\n",
      "feature w/o additional filter\n",
      "date: 2021-10-25, methane and weather data merged successfully in 2.0453929901123047 seconds\n",
      "merging variable:  integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation\n",
      "feature w/ additional filter\n",
      "date: 2021-10-25, methane and weather data merged successfully in 2.2291617393493652 seconds\n",
      "merging variable:  precipitation_amount_1hour_Accumulation\n",
      "feature w/ additional filter\n",
      "date: 2021-10-25, methane and weather data merged successfully in 2.3971874713897705 seconds\n",
      "There are 19 objects available for October, 2021\n",
      "--\n",
      "2021/10/data/air_pressure_at_mean_sea_level.nc\n",
      "2021/10/data/air_temperature_at_2_metres.nc\n",
      "2021/10/data/air_temperature_at_2_metres_1hour_Maximum.nc\n",
      "2021/10/data/air_temperature_at_2_metres_1hour_Minimum.nc\n",
      "2021/10/data/dew_point_temperature_at_2_metres.nc\n",
      "2021/10/data/eastward_wind_at_100_metres.nc\n",
      "2021/10/data/eastward_wind_at_10_metres.nc\n",
      "2021/10/data/integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation.nc\n",
      "2021/10/data/lwe_thickness_of_surface_snow_amount.nc\n",
      "2021/10/data/northward_wind_at_100_metres.nc\n",
      "2021/10/data/northward_wind_at_10_metres.nc\n",
      "2021/10/data/precipitation_amount_1hour_Accumulation.nc\n",
      "2021/10/data/sea_surface_temperature.nc\n",
      "2021/10/data/sea_surface_wave_from_direction.nc\n",
      "2021/10/data/sea_surface_wave_mean_period.nc\n",
      "2021/10/data/significant_height_of_wind_and_swell_waves.nc\n",
      "2021/10/data/snow_density.nc\n",
      "2021/10/data/surface_air_pressure.nc\n",
      "2021/10/main.nc\n",
      "\n",
      "List of variables to download: ['air_pressure_at_mean_sea_level', 'air_temperature_at_2_metres', 'air_temperature_at_2_metres_1hour_Maximum', 'air_temperature_at_2_metres_1hour_Minimum', 'dew_point_temperature_at_2_metres', 'eastward_wind_at_100_metres', 'eastward_wind_at_10_metres', 'integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation', 'lwe_thickness_of_surface_snow_amount', 'northward_wind_at_100_metres', 'northward_wind_at_10_metres', 'precipitation_amount_1hour_Accumulation', 'snow_density', 'surface_air_pressure']\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  air_pressure_at_mean_sea_level\n",
      "time to extract data (seconds):  65.78830242156982\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  air_temperature_at_2_metres\n",
      "time to extract data (seconds):  53.247432470321655\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  air_temperature_at_2_metres_1hour_Maximum\n",
      "time to extract data (seconds):  53.92428135871887\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  air_temperature_at_2_metres_1hour_Minimum\n",
      "time to extract data (seconds):  53.61575961112976\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  dew_point_temperature_at_2_metres\n",
      "time to extract data (seconds):  56.271172761917114\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  eastward_wind_at_100_metres\n",
      "time to extract data (seconds):  58.42302632331848\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  eastward_wind_at_10_metres\n",
      "time to extract data (seconds):  57.85147786140442\n",
      "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
      "folder for storing weather data already exist, we're good!\n",
      "Start Extraction:  integral_wrt_time_of_surface_direct_downwelling_shortwave_flux_in_air_1hour_Accumulation\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "\n",
    "show_time = True\n",
    "\n",
    "#Run S5P Methane Data Extraction\n",
    "start_s5p_extraction(date_batches)\n",
    "methane_time=time.time()\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "if show_time:\n",
    "    print(\"Time to Finish Methane Data Extraction: \", methane_time-start)\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "#Run ERA5 Weather Data Extraction\n",
    "start_era5_extraction(date_batches)\n",
    "weather_time=time.time()\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "if show_time:\n",
    "    print(\"Time to Finish Weather Data Extraction: \", weather_time-methane_time)\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "#Run CA Climate Zone Data Addition\n",
    "start_CACZ_addition(date_batches)\n",
    "cacz_time=time.time()\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "if show_time:\n",
    "    print(\"Time to Finish CA ClimateZone Data Addition: \", cacz_time-weather_time)\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "#Run Data Pre Process to Group and Merge Data in Date Range\n",
    "data_pre_process(date_batches)\n",
    "preprocess_time=time.time()\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "if show_time:\n",
    "    print(\"Time to Pre Processing: \", preprocess_time-cacz_time)\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "#Run Merging of New Dataframe to Old Dataframe\n",
    "merge_final(date_batches)\n",
    "merge_final_time=time.time()\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "if show_time:\n",
    "    print(\"Time to Merge Final: \", merge_final_time-preprocess_time)\n",
    "print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "\n",
    "print(\"Total Time: \", time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5+5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare new data with old data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket = 'methane-capstone'\n",
    "# subfolder = 'data/pipeline-raw-data'\n",
    "# s3_path = bucket+'/'+subfolder \n",
    "# file_name='2021-10-16_2021-10-31_combined_data.parquet.gzip'\n",
    "# data_location = 's3://{}/{}'.format(s3_path, file_name)\n",
    "# new_dataset = pd.read_parquet(data_location)\n",
    "# new_dataset['time_utc'] = pd.to_datetime(new_dataset['time_utc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucket = 'methane-capstone'\n",
    "# subfolder = 'data/combined-raw-data'\n",
    "# s3_path = bucket+'/'+subfolder \n",
    "# file_name='data-zone-combined.parquet.gzip'\n",
    "# data_location = 's3://{}/{}'.format(s3_path, file_name)\n",
    "# current_dataset = pd.read_parquet(data_location)\n",
    "# current_dataset['time_utc'] = pd.to_datetime(current_dataset['time_utc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# day=\"2021-10-16\"\n",
    "# new_dataset[new_dataset['time_utc']== day].sort_values(by='BZone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_dataset[current_dataset['time_utc']== day].sort_values(by='BZone').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "lcc_arn": "arn:aws:sagemaker:us-west-2:020948580909:studio-lifecycle-config/install-python-packages-v4"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
