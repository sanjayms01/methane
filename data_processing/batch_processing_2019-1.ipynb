{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "This notebook has the full pipeline to parse Sentinel 5p data from AWS s3.\n",
    "\n",
    "\n",
    "* Partition will be by **time**\n",
    "* `.parquet` format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install steps\n",
    "\n",
    "You may need to do the following pip installs\n",
    "\n",
    "1. `!pip install xarray`\n",
    "2. `!pip install geopandas`\n",
    "3. `!pip install shapely`\n",
    "4. `!pip install netCDF4`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xarray\n",
    "#!pip install geopandas\n",
    "#!pip install shapely\n",
    "#!pip install netCDF4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODOS:\n",
    "* Check speed on a couple different machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All libraries proeprly loaded!! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "import sys\n",
    "import subprocess\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from collections import Counter\n",
    "try:\n",
    "\n",
    "    from matplotlib import pyplot as plt #viz\n",
    "    import matplotlib.colors as colors #colors for viz\n",
    "    import xarray as xr #process NetCDF\n",
    "    import numpy as np\n",
    "    import pandas as pd #data manipulation\n",
    "    import matplotlib.gridspec as gridspec #create subplot\n",
    "    from glob import iglob #data access in file manager\n",
    "    from os.path import join \n",
    "    from functools import reduce #string manipulation\n",
    "    import itertools #dict manipulation\n",
    "    import matplotlib.patches as mpatches\n",
    "    \n",
    "    from datetime import datetime, timedelta\n",
    "    import time\n",
    "    import pytz\n",
    "    \n",
    "    \n",
    "    #GeoPandas\n",
    "    import geopandas as gpd\n",
    "    from shapely.geometry import Point\n",
    "    from shapely.geometry.polygon import Polygon\n",
    "    \n",
    "except ModuleNotFoundError:\n",
    "\n",
    "    print('\\nModule import error', '\\n')\n",
    "    print(traceback.format_exc())\n",
    "\n",
    "else:\n",
    "    print('\\nAll libraries proeprly loaded!!', '\\n')\n",
    "\n",
    "### HELPER FUNCTIONS\n",
    "def getHumanTime(seconds):\n",
    "    '''Make seconds human readable'''\n",
    "    if seconds <= 1.0:\n",
    "        return '00:00:01'\n",
    "    \n",
    "    seconds = int(seconds)\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return '{:d}:{:02d}:{:02d}'.format(h, m, s) # Python 3\n",
    "\n",
    "\n",
    "def print_write(content, f_object, should_print=True):\n",
    "    '''This function will both, and write the content to a local file'''\n",
    "    if should_print: print(content)\n",
    "    if f_object:\n",
    "        f_object.write(content)\n",
    "        f_object.write('\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in CA GeoJSON --> Polygon\n",
    "* Check point in Polygon - https://stackoverflow.com/questions/43892459/check-if-geo-point-is-inside-or-outside-of-polygon\n",
    "* Link to GeoJSON - https://github.com/ropensci/geojsonio/blob/master/inst/examples/california.geojson\n",
    "\n",
    "We will use this to strictly filter data only for California"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            geometry\n",
      "0  POLYGON ((48.97705 56.40782, 49.06494 55.07837...\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"48.824999999999996 54.94828910701515 3.5121093750000014 2.1327551465799104\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,112.02933336061021)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"0.07024218750000003\" opacity=\"0.6\" d=\"M 48.97705078125,56.4078233698268 L 49.06494140625,55.07836723201515 L 52.20703125,55.1286490684888 L 52.086181640625,56.480695390196296 L 52.020263671875,56.95096612859506 L 48.95507812499999,56.626020608371924 L 48.97705078125,56.4078233698268 z\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.polygon.Polygon at 0x7ff1481a4e90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_geo_path = \"/root/methane/data_processing/resources/testsite2.geojson\"\n",
    "ca_geo_file = open(ca_geo_path)\n",
    "ca_gpd = gpd.read_file(ca_geo_file)\n",
    "cali_polygon = ca_gpd['geometry'][0]\n",
    "print(ca_gpd)\n",
    "cali_polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check \n",
    "\n",
    "Do a quick check to ensure that the point checking is working "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT in California\n",
      "NOT in California\n",
      "NOT in California\n",
      "NOT in California\n"
     ]
    }
   ],
   "source": [
    "def inCalifornia(lat,lon):\n",
    "    '''Given lat, lon. Return Boolean if in California'''\n",
    "    point = Point(lon, lat)\n",
    "    return cali_polygon.contains(point)\n",
    "\n",
    "#Point in Arizona\n",
    "az_y= 32.866806\n",
    "az_x = -114.35925\n",
    "print(\"In California\") if inCalifornia(az_y, az_x) else print(\"NOT in California\")\n",
    "\n",
    "#Point in California\n",
    "ca_y = 37.962030\n",
    "ca_x = -121.186863\n",
    "print(\"In California\") if inCalifornia(ca_y, ca_x) else print(\"NOT in California\")\n",
    "\n",
    "\n",
    "#Point in Lake Tahoe, CA (border)\n",
    "ta_y = 38.913072\n",
    "ta_x = -119.913452\n",
    "print(\"In California\") if inCalifornia(ta_y, ta_x) else print(\"NOT in California\")\n",
    "\n",
    "#Point in Carson City, NV (border)\n",
    "cars_y = 39.155575\n",
    "cars_x = -119.721257\n",
    "print(\"In California\") if inCalifornia(cars_y, cars_x) else print(\"NOT in California\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get FULL list of CA orbit scanlines\n",
    "\n",
    "This comes from Jaclyn's selections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scan files: 2958\n"
     ]
    }
   ],
   "source": [
    "#Jaclyn's File\n",
    "#with open('/root/methane/data_processing/data_maps/ca_filenames.pickle', 'rb') as handle:\n",
    "#    ca_files = pickle.load(handle)    \n",
    "#print(\"Number of scan files:\", len(ca_files))\n",
    "\n",
    "#AWS OFFL Data Catalog\n",
    "#with open('/root/methane/data_processing/data_maps/data_catalog.pickle', 'rb') as handle:\n",
    "#    data_catalog = pickle.load(handle)   \n",
    "#print(\"Number of years:\", len(data_catalog))\n",
    "\n",
    "#Blair's File\n",
    "with open('./resources/nc_file_list.txt', 'rt') as handle:\n",
    "    ca_files = lines = [line.strip() for line in handle]    \n",
    "print(\"Number of scan files:\", len(ca_files))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check File Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'OFFL': 2958})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_types = []\n",
    "for fn_str in ca_files:\n",
    "    file_type = fn_str.split(\"_\")[1]\n",
    "    file_types.append(file_type)\n",
    "\n",
    "Counter(file_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get processing window --> `cur_file_window`\n",
    "\n",
    "Specify\n",
    "* `start_dt` - earliest can be 11/28/2018\n",
    "* `end_dt`- latest can be 10/01/2021, but....lets stick to 09/30/2021\n",
    "\n",
    "#### Would be best to do 4 different runs of the below code cell, one for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files:  113\n",
      "Number of batches:  9\n",
      "Number of files in batch: ~ 13\n"
     ]
    }
   ],
   "source": [
    "start_dt = datetime(2019, 6, 4).strftime('%Y%m%d')\n",
    "end_dt = datetime(2019, 6, 30).strftime('%Y%m%d')\n",
    "batch_size = 13\n",
    "\n",
    "cur_file_window = []\n",
    "non_offl_files = []\n",
    "\n",
    "#Ensure this is true!\n",
    "assert start_dt <= end_dt\n",
    "\n",
    "def getBatches(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "        \n",
    "        \n",
    "#Form Batches\n",
    "for ca_file in ca_files:\n",
    "    collection_date = ca_file.split(\"____\")[1].split(\"T\")[0]\n",
    "    if collection_date >= start_dt and collection_date <= end_dt:\n",
    "\n",
    "        #Only look at offline files...? \n",
    "        if \"OFFL\" in ca_file:\n",
    "            cur_file_window.append(ca_file)\n",
    "\n",
    "        else:\n",
    "            non_offl_files.append(ca_file)\n",
    "\n",
    "cur_file_window = sorted(cur_file_window, reverse=False) #oldest --> newest\n",
    "file_batches = list(getBatches(cur_file_window, batch_size))\n",
    "\n",
    "\n",
    "print(\"Number of files: \", len(cur_file_window))\n",
    "if len(non_offl_files) > 0: print(\"Number of files, can NOT use: \", len(non_offl_files), '\\n')\n",
    "print(\"Number of batches: \", len(file_batches))\n",
    "print(\"Number of files in batch: ~\", batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['S5P_OFFL_L2__CH4____20190604T070728_20190604T084858_08497_01_010301_20190610T083731.nc',\n",
       " 'S5P_OFFL_L2__CH4____20190604T084858_20190604T103028_08498_01_010301_20190610T104549.nc',\n",
       " 'S5P_OFFL_L2__CH4____20190604T203928_20190604T222058_08505_01_010301_20190610T224823.nc',\n",
       " 'S5P_OFFL_L2__CH4____20190604T222058_20190605T000228_08506_01_010301_20190610T235110.nc',\n",
       " 'S5P_OFFL_L2__CH4____20190605T064828_20190605T082958_08511_01_010301_20190611T135551.nc',\n",
       " 'S5P_OFFL_L2__CH4____20190605T082958_20190605T101128_08512_01_010301_20190611T102156.nc',\n",
       " 'S5P_OFFL_L2__CH4____20190605T202027_20190605T220157_08519_01_010301_20190611T222300.nc',\n",
       " 'S5P_OFFL_L2__CH4____20190605T220157_20190605T234327_08520_01_010301_20190611T234922.nc',\n",
       " 'S5P_OFFL_L2__CH4____20190605T234327_20190606T012457_08521_01_010301_20190612T011941.nc',\n",
       " 'S5P_OFFL_L2__CH4____20190606T062927_20190606T081057_08525_01_010301_20190612T081048.nc',\n",
       " 'S5P_OFFL_L2__CH4____20190606T081057_20190606T095227_08526_01_010301_20190612T100347.nc',\n",
       " 'S5P_OFFL_L2__CH4____20190606T095227_20190606T113357_08527_01_010301_20190612T113513.nc',\n",
       " 'S5P_OFFL_L2__CH4____20190606T214256_20190606T232426_08534_01_010301_20190612T232251.nc']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Steps\n",
    "<em> All files in `cur_file_window` will be processed.<em>\n",
    "\n",
    "**NOTE**\n",
    "For every batch we will write a file. Therefore we will **NOT** have a month wise split for data right now. In a post processign step we will aggregate by month for ease of access. \n",
    "\n",
    "Steps:\n",
    "1. Define the date window \n",
    "2. Setup file batches to download\n",
    "3. Download whole batch to local\n",
    "4. Parse each file in batch, and build a dataframe for each batch\n",
    "5. For every batch we will flush the data to a `.parquet.gzip` straight to S3 in respective folder.\n",
    "    * Format `startdt_enddt_meth.parquet.gzip`\n",
    "6. Delete contents inside Sagemaker batch folder where `.nc` files were downloaded for a respective batch\n",
    "7. Write out a `.txt` file that writes out all the steps that happened in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######### PIPELINE STARTED #########\n",
      "Total Files: 113\n",
      "Total Batches: 9\n",
      "Batch size: ~ 13\n",
      "batch_num: 1 - start NC files load\n",
      "batch_num: 1 - end NC files load, time_taken: 0:00:04\n",
      "\tfile_num: 1 - 20190604T070728::20190604T084858. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (133, 15), time_taken: 0:00:20\n",
      "\tfile_num: 2 - 20190604T084858::20190604T103028. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (197, 15), time_taken: 0:00:19\n",
      "\tfile_num: 3 - 20190604T203928::20190604T222058. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:19\n",
      "\tfile_num: 4 - 20190604T222058::20190605T000228. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:19\n",
      "\tfile_num: 5 - 20190605T064828::20190605T082958. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:19\n",
      "\tfile_num: 6 - 20190605T082958::20190605T101128. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (62, 15), time_taken: 0:00:19\n",
      "\tfile_num: 7 - 20190605T202027::20190605T220157. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:19\n",
      "\tfile_num: 8 - 20190605T220157::20190605T234327. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:19\n",
      "\tfile_num: 9 - 20190605T234327::20190606T012457. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:19\n",
      "\tfile_num: 10 - 20190606T062927::20190606T081057. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:19\n",
      "\tfile_num: 11 - 20190606T081057::20190606T095227. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (8, 15), time_taken: 0:00:19\n",
      "\tfile_num: 12 - 20190606T095227::20190606T113357. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:19\n",
      "\tfile_num: 13 - 20190606T214256::20190606T232426. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:19\n",
      "batch_num: 1 batch_total_time: 0:04:21 download_time: 00:00:01, file_count: 13, load_nc_time: 0:00:04, process_time: 0:04:16, cur_batch_df_shape: (400, 15)\n",
      "#######################\n",
      "batch_num: 2 - start NC files load\n",
      "batch_num: 2 - end NC files load, time_taken: 0:00:03\n",
      "\tfile_num: 1 - 20190606T232426::20190607T010556. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:21\n",
      "\tfile_num: 2 - 20190607T075156::20190607T093326. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (437, 15), time_taken: 0:00:22\n",
      "\tfile_num: 3 - 20190607T093326::20190607T111456. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (10, 15), time_taken: 0:00:22\n",
      "\tfile_num: 4 - 20190607T230525::20190608T004655. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:19\n",
      "\tfile_num: 9 - 20190618T074538::20190618T092708. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (194, 15), time_taken: 0:00:19\n",
      "\tfile_num: 10 - 20190618T092708::20190618T110837. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:19\n",
      "\tfile_num: 11 - 20190618T211736::20190618T225906. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:19\n",
      "\tfile_num: 12 - 20190618T225906::20190619T004036. start_parse.\n",
      "\t\t\t\t\tend_parse. shape: (0, 15), time_taken: 0:00:19\n",
      "\tfile_num: 13 - 20190619T072635::20190619T090805. start_parse.\n"
     ]
    }
   ],
   "source": [
    "#### CONFIG ####\n",
    "pipeline_start = True\n",
    "\n",
    "local_pipe_log_path = '/root/methane/data_processing/pipeline_runs/'\n",
    "#local_path = '/root/methane/data_processing/nc_data/cur_batch/'\n",
    "local_path = '/root/methane/data_processing/L2_data/'\n",
    "bucket = 'methane-capstone'\n",
    "subfolder = 'batch-raw-data'\n",
    "s3_path = bucket+'/data/testsite2/'+subfolder\n",
    "\n",
    "delete_local_files = False\n",
    "cur_file_window_set = set(cur_file_window)\n",
    "qa_thresh = 0.0\n",
    "\n",
    "\n",
    "#### HELPERS ####\n",
    "def getNCFile(batch_file_name, batch_file_num, local_path, outF):\n",
    "    '''Function to get a S5P file from AWS S3 to local'''\n",
    "    start = time.time()\n",
    "    try:\n",
    "        collection_date = batch_file_name.split(\"____\")[1].split(\"T\")[0]\n",
    "        year = collection_date[:4]\n",
    "        month = collection_date[4:6]\n",
    "        day = collection_date[6:]\n",
    "\n",
    "        command = ['aws','s3','cp', \n",
    "                   f's3://meeo-s5p/OFFL/L2__CH4___/{year}/{month}/{day}/{batch_file_name}',\n",
    "                   local_path]\n",
    "        print_write(f\"\\tbatch_file_num: {batch_file_num} \" + \" \".join(command), outF)\n",
    "        subprocess.check_output(command)\n",
    "    except:\n",
    "        print_write(f\"ISSUE GETTING: {batch_file_name}\", outF)\n",
    "    end = time.time()\n",
    "\n",
    "\n",
    "def getInputFiles(local_path):\n",
    "    '''Get list of input files'''\n",
    "    input_files = sorted(list(iglob(join(local_path, '**', '*CH4*.nc' ), recursive=True)))\n",
    "    return input_files\n",
    "\n",
    "def loadNCFiles(input_files, outF):\n",
    "    '''Use xarray to load .nc file S5p products'''    \n",
    "    start = time.time()\n",
    "    s5p_products = {}\n",
    "    for file_name in input_files:\n",
    "        name = file_name.split('/')[-1]\n",
    "        if name in cur_file_window_set:\n",
    "            start_date, end_date = list(filter(lambda x: len(x) == 15, file_name.split(\"_\")))\n",
    "            key = start_date + '::' + end_date\n",
    "            try:\n",
    "                #Open product - PRODUCT\n",
    "                with xr.load_dataset(local_path+file_name, group='PRODUCT') as s5p_prod:\n",
    "                    s5p_products[key] = s5p_prod\n",
    "                s5p_prod.close()\n",
    "            except:\n",
    "                print_write(f\"loadNCFiles - FAILED: {key}\", outF)\n",
    "    end = time.time()\n",
    "    return s5p_products, getHumanTime(end-start)\n",
    "\n",
    "def filter_in_cali(row):\n",
    "    '''Filter apply function for CA'''\n",
    "    point = Point(row['lon'], row['lat'])\n",
    "    return cali_polygon.contains(point)\n",
    "\n",
    "def processFiles(s5p_products, qa_thresh, outF):\n",
    "    '''Process all files that are in keys in `s5p_products` '''\n",
    "    columns = ['time_utc', \n",
    "               'lat', 'lon', \n",
    "               'rn_lat_1','rn_lon_1',\n",
    "               'rn_lat_2','rn_lon_2',\n",
    "               'rn_lat_5','rn_lon_5',\n",
    "               'rn_lat','rn_lon',\n",
    "               'qa_val', \n",
    "               'methane_mixing_ratio',\n",
    "               'methane_mixing_ratio_precision',\n",
    "               'methane_mixing_ratio_bias_corrected']\n",
    "    df_ca = pd.DataFrame(columns=columns)\n",
    "\n",
    "    init_start = time.time()\n",
    "    for file_number, product_key in enumerate(s5p_products, 1):\n",
    "        start_time = time.time()\n",
    "        s5p_prod = s5p_products[product_key]\n",
    "        df_cur_scan = pd.DataFrame(columns=columns)\n",
    "        times_utc = np.array(s5p_prod.time_utc[0, :])\n",
    "\n",
    "        print_write(f'\\tfile_num: {file_number} - {product_key}. start_parse.', outF)\n",
    "        \n",
    "        times_array = []\n",
    "        for utc, qa in zip(times_utc, s5p_prod.qa_value[0, :, :]):\n",
    "            times_array.extend([utc] * len(qa))\n",
    "\n",
    "        lats = np.array(s5p_prod.latitude[0, :, :]).ravel()\n",
    "        lons = np.array(s5p_prod.longitude[0, :, :]).ravel()\n",
    "        qa_vals = np.nan_to_num(s5p_prod.qa_value[0, :, :], nan=-9999).ravel()\n",
    "        methane_mixing_ratio = np.nan_to_num(s5p_prod.methane_mixing_ratio[0, :, :], nan=-9999).ravel()\n",
    "        methane_mixing_ratio_precision = np.nan_to_num(s5p_prod.methane_mixing_ratio_precision[0, :, :], nan=-9999).ravel()\n",
    "        methane_mixing_ratio_bias_corrected = np.nan_to_num(s5p_prod.methane_mixing_ratio_bias_corrected[0, :, :], nan=-9999).ravel()\n",
    "\n",
    "        cur_ts_dict = {\n",
    "            'time_utc' : times_array,\n",
    "            'lat' : lats,\n",
    "            'lon' : lons,\n",
    "            'rn_lat_1': np.round(lats, 1),\n",
    "            'rn_lon_1': np.round(lons, 1),\n",
    "            'rn_lat_2': np.round(lats*5)/5,\n",
    "            'rn_lon_2': np.round(lons*5)/5,\n",
    "            'rn_lat_5':  np.round(lats*2)/2,\n",
    "            'rn_lon_5': np.round(lons*2)/2,\n",
    "            'rn_lat': np.round(lats, 0),\n",
    "            'rn_lon': np.round(lons, 0),\n",
    "            'qa_val' : qa_vals,\n",
    "            'methane_mixing_ratio' : methane_mixing_ratio,\n",
    "            'methane_mixing_ratio_precision' : methane_mixing_ratio_precision,\n",
    "            'methane_mixing_ratio_bias_corrected' : methane_mixing_ratio_bias_corrected,\n",
    "        }\n",
    "\n",
    "        df_cur_ts = pd.DataFrame(cur_ts_dict)\n",
    "        df_cur_scan = pd.concat([df_cur_scan, df_cur_ts], ignore_index=True)\n",
    "\n",
    "        #QA Mask\n",
    "        qa_mask_df = df_cur_scan['qa_val'] >= qa_thresh\n",
    "\n",
    "        #Methane Mask\n",
    "        meth_ca_mask_df = (df_cur_scan.methane_mixing_ratio != -9999) & \\\n",
    "                          (df_cur_scan.methane_mixing_ratio_precision != -9999) & \\\n",
    "                          (df_cur_scan.methane_mixing_ratio_bias_corrected != -9999)\n",
    "\n",
    "        #California Geo Mask\n",
    "        cali_mask = df_cur_scan.apply(filter_in_cali, axis=1)\n",
    "\n",
    "        #Join Masks\n",
    "        mask_join_df = qa_mask_df & cali_mask & meth_ca_mask_df\n",
    "        df_cur_scan_masked = df_cur_scan[mask_join_df]\n",
    "        \n",
    "        df_ca = pd.concat([df_ca, df_cur_scan_masked], ignore_index=True)\n",
    "        end_time = time.time()\n",
    "        print_write(f'\\t\\t\\t\\t\\tend_parse. shape: {df_cur_scan_masked.shape}, time_taken: {getHumanTime(end_time-start_time)}', outF)\n",
    "        \n",
    "    return df_ca, getHumanTime(end_time-init_start)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### \n",
    "###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### \n",
    "###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### ###### \n",
    "\n",
    "if pipeline_start:\n",
    "    \n",
    "    #Give the pipeline log file a name\n",
    "    pipe_start = cur_file_window[0].split(\"____\")[1].split(\"T\")[0]\n",
    "    pipe_end = cur_file_window[-1].split(\"____\")[1].split(\"T\")[0]    \n",
    "    \n",
    "    outF = open(f\"{local_pipe_log_path}pipe_log_{pipe_start}_{pipe_end}.txt\", \"w\")\n",
    "    \n",
    "    init = time.time()\n",
    "    print_write(\"######### PIPELINE STARTED #########\", outF)\n",
    "    print_write(f\"Total Files: {len(cur_file_window)}\", outF)\n",
    "    print_write(f\"Total Batches: {len(file_batches)}\", outF)\n",
    "    print_write(f\"Batch size: ~ {batch_size}\", outF)\n",
    "    \n",
    "    try:\n",
    "\n",
    "        for batch_num, batch in enumerate(file_batches, 1):\n",
    "\n",
    "            #print_write(f\"\\nbatch_num: {batch_num} - start download\", outF)\n",
    "            batch_dn_start = time.time()\n",
    "            #for batch_file_num, batch_file in enumerate(batch, 1):\n",
    "\n",
    "                #Download one file\n",
    "            #    getNCFile(batch_file, batch_file_num, local_path, outF)\n",
    "                #continue\n",
    "\n",
    "            batch_dn_end = time.time()\n",
    "            #print_write(f\"batch_num: {batch_num} - finish download, time_taken: {getHumanTime(batch_dn_end-batch_dn_start)}\", outF)\n",
    "\n",
    "            #Get list of input files\n",
    "            #input_files = getInputFiles(local_path)\n",
    "            input_files = batch\n",
    "\n",
    "            #Load NC Files into dictionary\n",
    "            print_write(f\"batch_num: {batch_num} - start NC files load\", outF)\n",
    "            s5p_products, load_nc_time = loadNCFiles(input_files, outF)\n",
    "            print_write(f\"batch_num: {batch_num} - end NC files load, time_taken: {load_nc_time}\", outF)\n",
    "\n",
    "            #One entire batch is processsed into `cur_df`\n",
    "            cur_batch_df, process_time = processFiles(s5p_products, qa_thresh, outF)\n",
    "\n",
    "            ### Write `cur_batch_df` for each batch to S3 ###\n",
    "            try:\n",
    "\n",
    "                first_day = batch[0].split(\"____\")[1].split(\"T\")[0]\n",
    "                last_day = batch[-1].split(\"____\")[1].split(\"T\")[0]\n",
    "                year = first_day[:4]\n",
    "\n",
    "                file_name=f'{first_day}_{last_day}_meth.parquet.gzip'\n",
    "                cur_batch_df.to_parquet('s3://{}/{}'.format(s3_path+f'/{year}', file_name), compression='gzip')\n",
    "\n",
    "            except Exception:\n",
    "                traceback.print_exc()\n",
    "                write_loc = 's3://{}/{}'.format(s3_path+f'/{year}', file_name)\n",
    "                print_write(f\"ERROR S3 WRITE: {write_loc}\", outF)\n",
    "\n",
    "            if delete_local_files:\n",
    "                for f in input_files:\n",
    "                    os.remove(local_path+f)\n",
    "                print_write(\"deleted nc files\", outF)\n",
    "\n",
    "            batch_end_time = time.time()\n",
    "\n",
    "            print_write(f\"batch_num: {batch_num} batch_total_time: {getHumanTime(batch_end_time - batch_dn_start)} download_time: {getHumanTime(batch_dn_end-batch_dn_start)}, file_count: {len(batch)}, load_nc_time: {load_nc_time}, process_time: {process_time}, cur_batch_df_shape: {cur_batch_df.shape}\", outF)\n",
    "            print_write(\"#######################\", outF)\n",
    "        fin = time.time()\n",
    "        print_write(f\"\\nTOTAL PIPELINE TIME: {getHumanTime(fin-init)}\", outF)\n",
    "        outF.close()\n",
    "\n",
    "    except Exception:\n",
    "        traceback.print_exc()\n",
    "        print_write(f\"\\n !!KABOOM!!\", outF)\n",
    "        outF.close()\n",
    "    \n",
    "else:\n",
    "    print(\"PIPELINE CLOSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Reading back data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_key = '20181221_20181228_meth.parquet.gzip'\n",
    "# data_location = 's3://{}/{}'.format(s3_path+'/2018', data_key)\n",
    "# test_df = pd.read_parquet(data_location)\n",
    "# print(test_df.shape)\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing Methane Mask\n",
    "# #Methane Mask\n",
    "# meth_ca_mask_df = (test_df.methane_mixing_ratio != -9999) & \\\n",
    "#                   (test_df.methane_mixing_ratio_precision != -9999) & \\\n",
    "#                   (test_df.methane_mixing_ratio_bias_corrected != -9999)\n",
    "# #Join Masks\n",
    "# df_2 = test_df[meth_ca_mask_df]\n",
    "# print(df_2.shape)\n",
    "# df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance Tracker\n",
    "# 2 vCPU + 4 GiB ---> 46.80413150787353"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.4xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
